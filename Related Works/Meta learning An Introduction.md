# Meta Learning: An Introduction 

> Meta-learning, also known as “learning to learn”, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 
>
> 1) learn an efficient distance metric (metric-based); 
>
> 2) use (recurrent) network with external or internal memory (model-based); 
>
> 3) optimize the model parameters explicitly for fast learning (optimization-based).

Meta learning的思想源于人类对自身学习过程的理解。传统机器学习算法在训练过程中需要feed大量的样本，并且当任务发生改变时，模型需要重新训练。然而对于人类来说，我们可以很快的理解新知识，例如会骑自行车的人可以很快上手摩托车，甚至不需要示范。Meta learning要解决的就是**学习一个可以快速学习的模型**，这就是所谓的'**learning to learn**'。

传统机器学习无外乎学会分类、回归等，是**学会执行任务**（输入到输出的映射），可以视作**low-level**的学习。而 Meta learning 则设想能够**掌握学习方法**，使各种任务都能融会贯通，这就是**high-level**的学习了。

因此，Meta learning 并不局限于哪一种机器学习方法，也不局限于任务，而是一种high-level learning的思想，因此可以用到supervised learning, reinforcement learning等中：

- 即使仅在没有猫的图片数据集中学习，也可以在观察了少数的猫之后学会识别图中的猫
- 游戏AI可以很快的学会玩新游戏
- 即使仅在平坦的环境中进行训练，微型机器人也可以在测试过程中在上坡表面完成所需的任务。

In this post, I focus on the case when each desired task is a supervised learning problem like image classification. And I will another post for meta-reinforcement learning and its applications in robotics. 

## Definition

### Task as sample

Meta learning之所以如此强大，得益于其核心思想是将一个数据集作为一个样本，即将task作为sample。在一个task数据集 $\mathcal{D}=\{(\mathbb{x}_i,y_i)\}$ 中，数据又划分为两部分: support set $S$ 和 prediction set/query set $B$，$\mathcal{D}=\langle S,B\rangle$。

以图像分类为例，对于一个 *K-shot N-class classification* task 来说，$S$ 包含N个类别的K个数据。

![few-shot-classification](./Meta learning An Introduction.assets/few-shot-classification.png)

那么该怎么训练这个模型呢？

1. 在标签集 $\mathcal{L}$ 中取子集 $L$, 将对应的数据分为 $S^L, B^L$，这两部分构成 $\mathcal{D}$
2. $S^L$ 作为 part of model input
3. 用 mini-batch 的 $B^L$ 计算 loss and update the model parameters through backpropagation

![image-20191226194740259](./Meta learning An Introduction.assets/image-20191226194740259.png)

上式中，黑色部分是supervised learning的基本公式，红色部分就是 Meta learning 的特点。与传统的pre-trained方法不同的是，pre-trained是要在子任务中进行fine-tuning，而 meta learning 是要胜任大部分的任务。

### Learner and Meta-Learner

In addition, another popular view of meta learning decomposes the model update into two stages:

1. 一个被称作'learner'的传统classifier $f_\theta$ 学习一个给定的task
2. 同时，一个optimizer $g_\phi$ 学习如何更新如何通过 $S$ 更新 learner 的参数

在更新过程中，需要同时更新 $\theta, \phi$.

![image-20191226200939808](./Meta learning An Introduction.assets/image-20191226200939808.png)

关于这方面的 meta learning，在 [Ref.2](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a) 中有很详细的解释，并且作者很用心的绘制了精美的动图。

![img](./Meta learning An Introduction.assets/1_AcaPiikZErVv_iFJzWekQg.gif)



### Common Approaches

不同的 meta learning 实现方式主要区别在如何对 $P_{\theta}(y|\mathbb{x})$ 进行建模

![image-20191226202326065](./Meta learning An Introduction.assets/image-20191226202326065.png)



## Metric-Based

类似于最近邻算法（kNN分类和k-means聚类）以及 [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation). The predicted probability over a set of known labels $y$ is a weighted sum of labels of support set samples. The weight is generated by a kernel function $k_\theta$, measuring the similarity between two data samples.
$$
P_{\theta}(y | \mathbf{x}, S)=\sum_{\left(\mathbf{x}_{i}, y_{i}\right) \in S} k_{\theta}\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}
$$
因此，这种方法的关键在于学习一个好的**核函数来度量样本之间的关系**。

下面介绍几种**核函数的设计方法**。

### Convolutional Siamese Neural Network

 [Siamese neural network](http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf) 将两个 input 分别进行 embed (比较好奇这里为什么要使用了两个twin embed NN)，再汇总到一个度量映射网络 to judge whether they are in the same class.

![siamese](./Meta learning An Introduction.assets/siamese-conv-net.png)

1. 学习 embedding function $f_\theta$
2. 两个 embedding 的L1距离（也可以是其他的度量） $L_1=\left|f_{\theta}\left(\mathbf{x}_{i}\right)-f_{\theta}\left(\mathbf{x}_{j}\right)\right|$ 
3. 通过一个NN将距离映射到两个属于同一class的概率 $p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\sigma\left(\mathbf{W}\left|f_{\theta}\left(\mathbf{x}_{i}\right)-f_{\theta}\left(\mathbf{x}_{j}\right)\right|\right)$
4. 由于label是二元的，因此loss采用交叉熵 $\mathcal{L}(B)=\sum_{\left(\mathbf{x}_{i}, \mathbf{x}_{i}, y_{i}, y_{j}\right) \in B} \mathbf{1}_{y_{i}=y_{j}} \log p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)+\left(1-\mathbf{1}_{y_{i}=y_{j}}\right) \log \left(1-p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)\right)$
5. 对给定的 support set $S$ 和测试图 $\mathbb{x}$ (也就是prediction set), 预测结果为：$\hat{c}_{S}(\mathbf{x})=c\left(\arg \max _{\mathbf{x}_{i} \in S} P\left(\mathbf{x}, \mathbf{x}_{i}\right)\right)$，即将与其度量最近的support set中的数据的类别作为测试图的类别

这种方法是不是听起来和最近邻也没什么区别？其实关键在于学习得到的embedding，我们期望它可以泛化到**未知**的类别中，这就是它比pre-trained的优势所在。

### Matching Networks

Matching Networks最早由 [Vinyals et al., 2016](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf) 提出

![siamese](./Meta learning An Introduction.assets/matching-networks.png)
$$
c_{S}(\mathbf{x})=P(y | \mathbf{x}, S)=\sum_{i=1}^{k} a\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}, \text { where } S=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}_{i=1}^{k}
$$
与Siamese Neural Network不同，Matching Networks中的核函数 $a(\mathbb{x}, \mathbb{x}_i)$ 采用了Attention Mechanism形式: 
$$
a\left(\mathbf{x}, \mathbf{x}_{i}\right)=\frac{\exp \left(\operatorname{cosine}\left(f(\mathbf{x}), g\left(\mathbf{x}_{i}\right)\right)\right.}{\sum_{j=1}^{k} \exp \left(\operatorname{cosine}\left(f(\mathbf{x}), g\left(\mathbf{x}_{j}\right)\right)\right.}
$$

- cosine 表示两个特征的cosine距离
- $f,g$ 为图中的 $f_\theta, g_\theta$ 特征提取器，这是matching networks的**核心部分**

#### Support set特征提取器 $g_\theta(\mathbb{x}_i, S)$

**核心思想：**support set 中 $\mathbb{x}$ 的特征不止与其本身有关，还应该与support set中其他的数据有关

因此，作者将无序的集合看成有序的，利用**双向LSTM**对 $g_\theta(\mathbb{x}_i, S)$ 进行建模
$$
\begin{aligned}
&\vec{h}_{i}, \vec{c}_{i}=\operatorname{LSTM}\left(g^{\prime}\left(x_{i}\right), \vec{h}_{i-1}, \vec{c}_{i-1}\right)\\
&\overleftarrow{h}_{i}, \overleftarrow{c}_{i}=\operatorname{LSTM}\left(g^{\prime}\left(x_{i}\right), \overleftarrow{h}_{i+1}, \overleftarrow{c}_{i+1}\right)\\
&g\left(x_{i}\right)=\vec{h}_{i}+\overleftarrow{h}_{i}+g^{\prime}\left(x_{i}\right), i=1,2 \ldots k
\end{aligned}
$$

> 其中`g'(x)`, `h(i-1)`, `c(i-1)`, `h(i+1)`, `c(i+1) `分别表示原始状态，上一时刻的隐含状态，上一时刻的记忆状态，下一时刻的隐含状态，下一时刻的记忆状态；

#### Prediction set特征提取器 $f_\theta(\mathbb{x}, S)$

通过**注意力LSTM**获取Prediction set的特征
$$
\begin{aligned}
\hat{h}_{k}, c_{k} &=\operatorname{LSTM}\left(f^{\prime}(\hat{x}),\left[h_{k-1}, r_{k-1}\right], c_{k-1}\right) \\
h_{k} &=\hat{h}_{k}+f^{\prime}(\hat{x}) \\
r_{k-1} &=\sum_{i=1}^{|S|} a\left(h_{k-1}, g\left(x_{i}\right)\right) g\left(x_{i}\right) \\
a\left(h_{k-1}, g\left(x_{i}\right)\right) &=e^{h_{k-1}^{T} g\left(x_{i}\right)} / \sum_{j=1}^{|S|} e^{h_{k-1}^{T} g\left(x_{j}\right)}
\end{aligned}
$$

1. Prediction set先经过一个basic NN (such as CNN)，获取基本特征 $f'(\mathbb{x})$
2. 将Support set的read attention vector $r_{k-1}$ 作为一部分隐变量进入LSTM训练
3. 经过K步后，$f_\theta(\mathbb{x},S)=h_K$

这种方法叫做'Full Contextual Embeddings (FCE)'。有趣的是，它在复杂任务中表现的比简单任务要好。

Matching Networks进一步完善了训练和测试的条件应该匹配的思想，训练过程被设计成能够模拟测试过程的推理。

### Relation Network

**Relation Network (RN)** ([Sung et al., 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf))类似于Siamese Neural Network，主要有以下几点不同：

1. 使用CNN训练的classifier $g_\phi$ 代替L1距离，$r_{ij}=g_\phi([\mathbb{x}_i,\mathbb{x}_j]), [.,.]$代表concatention
2. Loss function使用了MSE取代交叉熵，这是因为RN更注重样本之间的relationship，更类似于回归问题，而不是Siamese Neural Network那样的分类问题

![relation-network](./Meta learning An Introduction.assets/relation-network.png)

### Prototypical Networks

**Prototypical Networks** ([Snell, Swersky & Zemel, 2017](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf)) 使用了**聚类**的思想，将每一个input编码成M维特征向量。对于同一类别中的所有特征向量，求出一个 *prototype feature* (也就是mean vector)

<img src="D:\Github\Reinforcement-Learning-in-Robotics\Related Works\Meta learning An Introduction.assets\prototypical-networks.png" alt="prototypical-networks" style="zoom:67%;" />

Prediction set的属于哪一个类别的概率就是根据其与各个class之间距离的倒数来定义的
$$
P(y=c | \mathbf{x})=\operatorname{softmax}\left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c}\right)\right)=\frac{\exp \left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c}\right)\right)}{\sum_{c^{\prime} \in \mathcal{C}} \exp \left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c^{\prime}}\right)\right)}
$$
至于距离的度量方法可以自己选取，文中选取的是欧氏距离。

Loss function采用 negative log-likelihood: $\mathcal{L}(\theta)=-\log P_{\theta}(y=c | \mathbf{x})$



## Optimization-Based 

我们知道传统的深度学习网络参数的更新，都是通过gradient backpropagation实现的。但是这种方式并不适用于少样本的情况（样本少，梯度方向无法表征总体梯度方向，易发散）。因此若想在少样本的情况下依然能够很好的学习，就需要在optimization方式上进行改变。这就是Optimization-Based meta-learning(就是上文所说的Learner and Meta-Learner)的思路。

### LSTM Meta-Learner

- Pytorch Code: https://github.com/markdtw/meta-learning-lstm-pytorch

不同于传统的公式化更新，Optimization 也可以作为Model进行精细化建模，[Ravi & Larochelle (2017)](https://openreview.net/pdf?id=rJY0-Kcll)将optimization model命名为 'Meta-learner'，而原来的model叫做 'Learner'。

Let’s denote the learner model as $M_\theta$ parameterized by $\theta$, the meta-learner as $R_\Theta$ with parameters $\Theta$, and the loss function $\mathcal{L}$.

#### 为什么是LSTM？

1. 作者发现LSTM的更新规则和一般的梯度下降算法更新规则非常类似
2. 知道梯度的历史有利于梯度的更新，就像动量一样

#### 算法思路

**gradient decent**对Learner的更新规则如下：
$$
\theta_{t}=\theta_{t-1}-\alpha_{t} \nabla_{\theta_{t-1}} \mathcal{L}_{t}
$$
**LSTM中单元状态（cell state）**的更新规则如下：
$$
\begin{equation}\label{6}
c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t}
\end{equation}
$$
如果LSTM中的遗忘门 $f_t=1$，输入门 $i_t=\alpha_t$，cell state $c_t=\theta_t$，new cell state $\tilde{c}_{t}=-\nabla_{\theta_{t-1}} \mathcal{L}_{t}$. 那么 Equation $\ref{6}$ 就可以**转变为梯度更新形式**：
$$
\begin{aligned}
c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\
&=\theta_{t-1}-\alpha_{t} \nabla_{\theta_{t-1}} \mathcal{L}_{t}
\end{aligned}
$$
是不是感觉idea的产生就是如此的简单！

当然，有了idea还需要完善。直接让遗忘门和输入门取定值会让LSTM失去了很多威力，因此作者进行了refine:
$$
\begin{aligned}
&f_{t}=\sigma\left(\mathbf{W}_{f} \cdot\left[\nabla_{\theta_{t-1}} \mathcal{L}_{t}, \mathcal{L}_{t}, \theta_{t-1}, f_{t-1}\right]+\mathbf{b}_{f}\right) \\ &\qquad\text {  how much to forget the old value of parameters   } \theta_{t-1}\\
&i_{t}=\sigma\left(\mathbf{W}_{i} \cdot\left[\nabla_{\theta_{t}, 1} \mathcal{L}_{t}, \mathcal{L}_{t}, \theta_{t-1}, i_{t-1}\right]+\mathbf{b}_{i}\right) \\ &\qquad\text {  corresponding to the learning rate at time step t. }\\
&\tilde{\theta}_{t}=-\nabla_{\theta_{t-1}} \mathcal{L}_{t}\\
&\theta_{t}=f_{t} \odot \theta_{t-1}+i_{t} \odot \tilde{\theta}_{t}
\end{aligned}
$$

#### 模型建立

![lstm-meta-learner](./Meta learning An Introduction.assets/lstm-meta-learner.png)

与之前介绍的Matching Networks一样，LSTM Meta-Learner也在训练过程中模拟测试环节。

1. Sample a training dataset $\mathcal{D}=\left(\mathcal{D}_{\text {support}}, \mathcal{D}_{\text {pred}}\right) \in \hat{\mathcal{D}}_{\text {meta-train }}$
2. 用 $\mathcal{D}_{\text {support }}$更新T轮 Learner 参数 $\theta$ 
3. 最后一个cell state，也就是 $\theta_T$ 被用来在 $\mathcal{D}_{\text {pred}}$ 上训练 Meta-Learner $ \Theta$

![train-meta-learner](./Meta learning An Introduction.assets/train-meta-learner.png)

#### Tricks

由于LSTM一般是用于深度网络的，参数众多。为避免参数爆炸，作者做了**参数共享**。具体思路是 sharing parameters across coordinates，详见 [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)。**简而言之，对于同一层中的参数 $\theta_i$，他们的更新规则相同，即权重和偏差 $W,b$ 相同。** 



### Model-Agnostic Meta-Learning(MAML)

- Pytorch Code：[dragen1860/MAML-Pytorch](https://link.zhihu.com/?target=https%3A//github.com/dragen1860/MAML-Pytorch) 或 [katerakelly/pytorch-maml](https://link.zhihu.com/?target=https%3A//github.com/katerakelly/pytorch-maml)
- Tensorflow Code：[cbfinn/maml](https://link.zhihu.com/?target=https%3A//github.com/cbfinn/maml)

Berkeley的 Finn 在meta-learning领域创造了很多开创性的成果，尤其是在meta-reinforcement learning in robotics方面，以后会详细讲解这方面的成果。

**Model-Agnostic Meta-Learning** ([Finn, et al. 2017](https://arxiv.org/abs/1703.03400)) 是一个非常通用的optimization algorithm，它可以附加在任何传统的基于梯度的神经网络模型上，所以叫做**model-agnostic**即**模型无关**。

#### 算法思路

假设model为 $f_\theta$，给定task $\tau_i$ 以及数据集 $(\mathcal{D}_{support}^{(i)}, \mathcal{D}_{pred}^{(i)})$，我们以往的更新方式是：
$$
\begin{equation}\label{7}
\theta_{i}^{\prime}=\theta-\alpha \nabla_{\theta} \mathcal{L}_{\tau_{i}}^{(0)}\left(f_{\theta}\right)
\end{equation}
$$
这里 $\mathcal{L}^{(0)}$ 代表id为0的mini data batch上的loss。

但是 Equation $\ref{7}$ 仅代表一个task上的优化，要想模型能够在不同的tasks上具有泛化性，我们需要找到最佳的 $\theta^*$。我们继续sample一个新的data batch id(1)，它的loss就是 $\mathcal{L}^{(1)}$，那么最优的 $\theta^* $ 就是：
$$
\begin{aligned}
\theta^{*} &=\arg \min _{\theta} \sum_{\tau_{i} \sim p(\tau)} \mathcal{L}_{\tau_{i}}^{(1)}\left(f_{\theta_{i}^{\prime}}\right)=\arg \min _{\theta} \sum_{\tau_{i} \sim p(\tau)} \mathcal{L}_{\tau_{i}}^{(1)}\left(f_{\theta-\alpha \nabla_{\theta} \mathcal{L}_{\tau_i}^{(0)}\left(f_{\theta}\right)}\right) \\
\theta & \leftarrow \theta-\beta \nabla_{\theta} \sum_{\tau_{i} \sim p(\tau)} \mathcal{L}_{\tau_{i}}^{(1)}\left(f_{\theta-\alpha \nabla_{\theta} \mathcal{L}_{\tau_{i}}^{(0)}\left(f_{\theta}\right)}\right)
\end{aligned}
$$
<img src="./Meta learning An Introduction.assets/maml-algo.png" alt="MAML Algorithm" style="zoom: 50%;" />

**解释一下：** 

1. $\mathcal{L}_{\tau_i}^{(j)}$ 下标是任务 $\tau_i\sim p(\tau)$, 上标是指在任务 $\tau_i $ 下sample的data batch id(j)。
2. $\theta_i'$ 代表在task $\tau_i$ 下利用 $\mathcal{D}_{support}$ 进行**第一次梯度更新**得到的**参数中间结果**，并**不直接作用**于model $f_\theta $的更新。
3. 在得到各个task的参数中间结果之后，在中间结果的基础上利用 $\mathcal{D}_{pred}$ 再进行**第二次梯度更新**，这次**直接作用**于model参数的更新。可以将这次梯度更新的方向理解为各个task**综合博弈的最优方向**。

下图展示的就是在三个任务上的**梯度子方向**，大的黑色箭头就是博弈后的**梯度主方向**，以上都是model **pre-trained**过程。当我们需要对某个task **fine-tuning**的时候，如图中的虚线，在pre-trained的参数基础上，经过few-shot就可以达到相应的最优参数。

<img src="./Meta learning An Introduction.assets/maml.png" alt="MAML" style="zoom:33%;" />

#### Tricks

由于MAML的计算用到了**二次梯度**，为避免提高计算复杂度，诞生了MAML的修改版：**First-Order MAML (FOMAML)**。

假设我们在第一次梯度更新进行 k 步，从初始参数 $\theta_{meta}$ 开始：
$$
\begin{aligned}
\theta_{0}&=\theta_{\text {meta }}\\
\theta_{1}&=\theta_{0}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{0}\right)\\
\theta_{2}&=\theta_{1}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{1}\right)\\
&...\\
\theta_{k}&=\theta_{k-1}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{k-1}\right)
\end{aligned}
$$
在第二次更新的时候就是：
$$
\theta_{\mathrm{meta}} \leftarrow \theta_{\mathrm{meta}}-\beta g_{\mathrm{MAML}}
$$
其中
$$
\begin{aligned}
g_{\mathrm{MAML}} &=\nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{k}\right) \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot\left(\nabla_{\theta_{k-1}} \theta_{k}\right) \ldots\left(\nabla_{\theta_{0}} \theta_{1}\right) \cdot\left(\nabla_{\theta} \theta_{0}\right) \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k} \nabla_{\theta_{-1}} \theta_{i} \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k} \nabla_{\theta_{-1}}\left(\theta_{i-1}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{i-1}\right)\right) \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k}\left(I-\alpha \nabla_{\theta_{i-1}}\left(\nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{i-1}\right)\right)\right)
\end{aligned}
$$
MAML的更新就是上式：
$$
\begin{equation}\label{9}
g_{\mathrm{MAML}}=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k}\left(I-\alpha \nabla_{\theta_{i-1}}\left(\nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{i-1}\right)\right)\right)
\end{equation}
$$
FOMAML直接忽略了 Equation $\ref{9}$ 中的二次梯度：
$$
g_{\mathrm{FOMAML}}=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right)
$$
对，就是这么简单粗暴，难算的我们就不要了...



### Reptile

- Tensorflow Code: https://github.com/openai/supervised-reptile

**Reptile** ([Nichol, Achiam & Schulman, 2018](https://arxiv.org/abs/1803.02999)) 是一种非常简单的optimization algorithm。它和MAML一样是基于梯度的并且也是model-agnostic。这是OpenAI的一项工作，可以读一下[OpenAI blog](https://openai.com/blog/reptile/)

#### 算法思路

由于MAML具有二次梯度，Reptile设法在**第二次梯度更新**时抛弃梯度的计算，而是使用一种更soft的方式得到**综合博弈方向**：
$$
\theta \leftarrow \theta+\alpha \frac{1}{n} \sum_{i=1}^{n}\left(W_{i}-\theta\right)
$$
<img src="D:\Github\Reinforcement-Learning-in-Robotics\Related Works\Meta learning An Introduction.assets\reptile-algo.png" alt="Reptile Algorithm" style="zoom: 25%;" />

#### Tricks

Reptile乍一看和普通的SGD没什么区别。当SGD的梯度更新step k=1时，reptile其实就是相当于在**全集**上进行梯度更新，但是这样的 'joint training' 可能会导致某些特定的task得不到关注。例如在zero-shot问题中，这样做就收效甚微。因此需要 k>1，使其包含二阶或更高阶微分项。

更多的理论分析详见 [Ref. 9](https://www.cnblogs.com/veagau/p/11816163.html) 和 [Ref. 1]([Ref. 9])

#### Reptile vs FOMAML

假设**第一次梯度更新**进行两步，即 k=2，像MAML里一样，它们的损失为 $\mathcal{L}^{(0)}, \mathcal{L}^{(1)}$

设 $g_{j}^{(i)}=\nabla_{\theta} \mathcal{L}^{(i)}\left(\theta_{j}\right)$, $H_{j}^{(i)}=\nabla_{\theta}^{2} \mathcal{L}^{(i)}\left(\theta_{j}\right)$
$$
\begin{equation}
\begin{aligned}
&\theta_{0}=\theta_{\text {meta }}\\
&\theta_{1}=\theta_{0}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{0}\right)=\theta_{0}-\alpha g_{0}^{(0)}\\
&\theta_{2}=\theta_{1}-\alpha \nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{1}\right)=\theta_{0}-\alpha g_{0}^{(0)}-\alpha g_{1}^{(1)}
\end{aligned}
\end{equation}\label{12}
$$
那么MAML, FOMAML, Reptile的**第二次更新**可以写作：
$$
\begin{equation}
\begin{aligned}
g_{\mathrm{FOMAML}} &=\nabla_{\theta_{1}} \mathcal{L}^{(1)}\left(\theta_{1}\right)=g_{1}^{(1)} \\
g_{\mathrm{MAML}} &=\nabla_{\theta_{1}} \mathcal{L}^{(1)}\left(\theta_{1}\right) \cdot\left(I-\alpha \nabla_{\theta}^{2} \mathcal{L}^{(0)}\left(\theta_{0}\right)\right)=g_{1}^{(1)}-\alpha H_{0}^{(0)} g_{1}^{(1)}\\
g_{\text {Reptile }}&=\left(\theta_{0}-\theta_{2}\right) / \alpha=g_{0}^{(0)}+g_{1}^{(1)}
\end{aligned}
\end{equation}\label{13}
$$
<img src="./Meta learning An Introduction.assets/reptile_vs_FOMAML.png" alt="Reptile vs FOMAML" style="zoom: 20%;" />

将 Equation $\ref{13}$ 进行泰勒展开，由于 $\theta_{1}-\theta_{0}=-\alpha g_{0}^{(0)}$
$$
\begin{aligned}
g_{1}^{(1)} &=\nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{1}\right) \\
&=\nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{0}\right)+\nabla_{\theta}^{2} \mathcal{L}^{(1)}\left(\theta_{0}\right)\left(\theta_{1}-\theta_{0}\right)+\frac{1}{2} \nabla_{\theta}^{3} \mathcal{L}^{(1)}\left(\theta_{0}\right)\left(\theta_{1}-\theta_{0}\right)^{2}+\ldots \\
&=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+\frac{\alpha^{2}}{2} \nabla_{\theta}^{3} \mathcal{L}^{(1)}\left(\theta_{0}\right)\left(g_{0}^{(0)}\right)^{2}+\ldots \\
&=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+O\left(\alpha^{2}\right)
\end{aligned}
$$
因此Equation $\ref{13}$可以写作：
$$
\begin{aligned}
g_{\mathrm{FOMAML}} &=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+O\left(\alpha^{2}\right) \\
g_{\mathrm{MAML}}&=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}-\alpha H_{0}^{(0)} g_{0}^{(1)}+O\left(\alpha^{2}\right)\\
g_{\text {Reptile }} &=g_{0}^{(0)}+g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+O\left(\alpha^{2}\right)
\end{aligned}
$$

- $A=\mathbb{E}_{\tau, 0,1}\left[g_{0}^{(0)}\right]=\mathbb{E}_{\tau, 0,1}\left[g_{0}^{(1)}\right]$ 代表各个tasks loss的平均梯度。A用于**提高task层次的性能**，又可称作 AvgGrad；
- $B=\mathbb{E}_{\tau, 0,1}\left[H_{0}^{(1)} g_{0}^{(0)}\right]=\frac{1}{2} \mathbb{E}_{\tau, 0,1}\left[H_{0}^{(1)} g_{0}^{(0)}+H_{0}^{(0)} g_{0}^{(1)}\right]=\frac{1}{2} \mathbb{E}_{\tau, 0,1}\left[\nabla_{\theta}\left(g_{0}^{(0)} g_{0}^{(1)}\right)\right]$ 代表每一个task内两个mini-batch的梯度方向的内积。B用于**提高data层次的性能**，又可称作 AvgGradInner。

$$
\begin{array}{l}
{\mathbb{E}_{\tau, 1,2}\left[g_{\text {FOMAML }}\right]=A-\alpha B+O\left(\alpha^{2}\right)} \\
{\mathbb{E}_{\tau, 1,2}\left[g_{\text {MAML }}\right]=A-2 \alpha B+O\left(\alpha^{2}\right)} \\
{\mathbb{E}_{\tau, 1,2}[{g_{ \text {Reptili} }}]=2 A-\alpha B+O\left(\alpha^{2}\right)}
\end{array}
$$

这样看三种方法的侧重点就很明显了。可以看到三者AvgGradInner与AvgGrad之间的系数比的关系是：**MAML > FOMAML > Reptile**。



## Model-Based

Model-Based与Metric-Based方法的区别在于不对 $P_\theta(y|\mathbb{x})$ 的形式做假设，而是依靠一个可以快速学习的model。model能够快速更新参数，得益于其内部结构，或由另一个元学习器模型来控制。也就是说，Model-Based方法是在寻找最优架构。

### Memory-Augmented Neural Networks(MANN)

既然 meta-learning 的目标是让其能够在过往知识的基础上，快速地学习新知识，那么给model加一个**memory存储器**就是很自然的想法了。这样的model被称作**Memory-Augmented Neural Networks(MANN)**。虽然RNN, LSTM这种循环神经网络也具有记忆，但是是内部记忆，而MANN就简单粗暴很多，类似于DQN的experience replay buffer。

[Neural Turing Machines](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines) (相关博客 [中](https://zhuanlan.zhihu.com/p/30383994) [英](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines)）和 [Memory Networks](https://arxiv.org/abs/1410.3916) (相关博客 [中](https://zhuanlan.zhihu.com/p/32257642)) 都是利用memory来促进学习过程的先驱。在神经图灵机的基础上，[Santoro et al. (2016)](http://proceedings.mlr.press/v48/santoro16.pdf) 对训练设置和内存检索机制做了一些修改，提出了一种新的‘寻址机制’，用于决定对如何对记忆向量分配注意力权重。

#### Neural Turing Machines

神经图灵机的结构如下图所示，它构建了一个**controller**用于学习如何通过 soft attention 读取和写入memory。Attention weights 是通过寻址机制生成的，寻址机制包含基于内容和基于位置两部分。

<img src="./Meta learning An Introduction.assets/NTM.png" alt="NTM" style="zoom: 25%;" />

#### MANN for Meta-Learning

![NTM](./Meta learning An Introduction.assets/mann-meta-learning.png)

如果要将memory network这种方法用到meta-learning中，就要做到以下两点：

- 存储的信息相对稳定
- 存储空间大小不受模型参数大小限制

由上图(a)知，这是一个LSTM网络，它以($\mathbf{x}_{t+1}, y_{t}$)的形式作为输入，即前一个timestep的true label在下一个timestep作为输入的一部分。MANN借由这种方式使memory保存更长时间，内存必须保留当前输入，直到以后出现标签，然后检索旧信息以进行相应的预测。如上图(b)，添加了external memory存储上一次的x输入，这使得下一次输入后进行反向传播时，可以让y label和x建立联系，使得之后的x能够通过外部记忆获取相关图像进行比对来实现更好的预测。图(b)中Bind and Encode阶段就是“写”操作，而Retrieve Bound Information阶段就是“读”操作。



下面从**read**和**write**两个主要部分介绍MANN：

**Read the memory**

Read attention 是基于**内容相似度**的，即对于每一时刻输入的 $\mathbf{x}_t$，先映射成特征向量 $k_t$，然后跟memory buffer中的记忆向量进行相似度比较，相似度大则给与较大的权重。

MANN使用的是**cosine similarity**，并用**softmax**进行正则化。最终读取的向量 $\mathbf{r}_t$ 就是memory的加权和：
$$
\mathbf{r}_{i}=\sum_{i=1}^{N} w_{t}^{r}(i) \mathbf{M}_{t}(i), \text { where } w_{t}^{r}(i)=\operatorname{softmax}\left(\frac{\mathbf{k}_{t} \cdot \mathbf{M}_{t}(i)}{\left\|\mathbf{k}_{t}\right\| \cdot\left\|\mathbf{M}_{t}(i)\right\|}\right)
$$
其中，$\mathbf{M}_t$ 表示记忆矩阵，$\mathbf{M}_t(i)$ 表示记忆矩阵的第i行。



**Write the memory**

**Least Recently Used Access**（LRUA）编写器是为MANN设计的，以便在元学习的情况下更好地工作。LRUA倾向于将新内容写到**使用最少**的存储位置或**最近使用**的存储位置。

- 取代使用最少：让memory buffer保留常用信息 (see [LFU](https://en.wikipedia.org/wiki/Least_frequently_used))
- 取代最近使用：这样做的动机是，一旦检索到一条信息，它可能会在一段时间内不会再次被调用 (see [MRU](https://en.wikipedia.org/wiki/Cache_replacement_policies#Most_recently_used_(MRU)))

为了实现上述写入目标，LRUA设计了一些权重：

- $\mathbf{w}_t^u$ 是使用权重，由读写权重 $\mathbf{w}_t^r, \mathbf{w}_t^w$ 决定，$\gamma$ 为折扣因子
- 读权重就不解释了，写在上面了
- 写权重上一timestep的读权重与上一timestep的least-used权重的插值
- least-used权重 $\mathbf{w}_t^{lu}$ 是由使用权重推导得来，

$$
\begin{aligned}
&w_{t}^{u} \leftarrow \gamma w_{t-1}^{u}+w_{t}^{r}+w_{t}^{w}\\
&w_{t}^{l u}=\left\{\begin{array}{ll}
{0} & {\text{if } w_{t}^{u}(i)>m\left(w_{t}^{u}, n\right)} \\
{1} & {\text{if } w_{t}^{u}(i) \leq m\left(w_{t}^{u}, n\right)}
\end{array}\right.\\
&w_{t}^{w} \leftarrow \sigma(\alpha) w_{t-1}^{r}+(1-\sigma(\alpha)) w_{t-1}^{l u}
\end{aligned}
$$

最后，记忆矩阵的更新方式如下：

$$
\mathbf{M}_{t}(i)=\mathbf{M}_{t-1}(i)+w_{t}^{w}(i) \mathbf{k}_{t}, \forall i
$$



### Meta Networks

**Meta Networks** ([Munkhdalai & Yu, 2017](https://arxiv.org/abs/1703.00837)), short for **MetaNet**，是一种用于跨任务快速泛化的体系结构和训练流程。

MetaNet 主要由 Fast weights 和 Slow weights 组成。

- Fast weights：用于跨任务的泛化。由于使用梯度更新的小样本学习速度慢，这里就设想使用更快的方式：**利用一个神经网络来生成另一个神经网络的参数**！
- Slow weights：指普通的基于SGD等优化的参数

在 MetaNet 中，**损失梯度信息**被作为 **meta information** ，用来生成fast weights。在神经网络中，将slow weights和fast weights结合起来进行预测。

<img src="./Meta learning An Introduction.assets\combine-slow-fast-weights.png" alt="slow-fast-weights" style="zoom:33%;" />

 其中，⨁是按元素求和。

#### Model Component

- 一个将input编码的embedding function $f_\theta$. 这跟  [Siamese Neural Network](https://zhuanlan.zhihu.com/p/99730942) 一样，用于判断两个样本是否属于同一class
- 一个base leaner model $g_\phi$. 

至此，这个模型和 [Relation Network](https://zhuanlan.zhihu.com/p/99730942) 没什么区别。

那么，区别在于，MetaNet 对上述两个model的模型进行了下图这样的**快速生成**！

<img src="./Meta learning An Introduction.assets/v2-2d61ff11eb1a5a9e52d6c12eb333eb4b_hd.jpg" alt="img" style="zoom: 80%;" />

所以我们还需要两个model $F,G$ 来生成 $f,g$ 的fast weights:

- $F_\omega$: 一个以 $\omega$ 为参数的LSTM. 它将 embedding function $f_\theta$ loss的梯度作为输入，来生成 $\theta^+$
- $G_{\nu}$: 一个以 $\nu$ 为参数的NN. 它将base learner的loss梯度作为输入，生成 $\phi^+$

了解了这些，终于可以看看 MetaNet 长什么样了。还是和之前一样，训练集由 support set $\mathcal{D}_{support}=\left\{\mathbf{x}_{i}^{\prime}, \boldsymbol{y}_{i}^{\prime}\right\}_{i=1}^{K}$ 和 prediction/query set $\mathcal{D}_{pred}=\left\{\mathbf{x}_{i}, {y}_{i}\right\}_{i=1}^{L}$ 构成，现在我们的模型包含了四个子模型，以及四种参数 $(\theta, \phi, \omega, \nu)$:

![meta-net](./Meta learning An Introduction.assets/meta-network.png)

#### Training Process

1. 每个timestep，从support set sample一对数据，利用embedding function求二者距离并计算loss: $\mathcal{L}_{t}^{\mathrm{emb}}=\mathbf{1}_{y_{i}^{\prime}=y_{j}^{\prime}} \log P_{t}+\left(1-\mathbf{1}_{y_{i}^{\prime}=y_{j}^{\prime}}\right) \log \left(1-P_{t}\right),$ where $P_{t}=\sigma\left(\mathbf{W}\left|f_{\theta}\left(\mathbf{x}_{(t, 1)}\right)-f_{\theta}\left(\mathbf{x}_{(t, 2)}\right)\right|\right)$

2. 计算task-level fast weights $\theta^{+}=F_{w}\left(\nabla_{\theta} \mathcal{L}_{1}^{\mathrm{emb}}, \ldots, \mathcal{L}_{T}^{\mathrm{emb}}\right)$

3. 便览support set中的数据，并计算example-level fast weights。同时，更新学习到的input表征：

   - Base learner输出一个概率分布：$P\left(\hat{y}_{i} | \mathbf{x}_{i}\right)=g_{\phi}\left(\mathbf{x}_{i}\right)$ 并用交叉熵计算损失$\mathcal{L}_{i}^{\text {task }}=y_{i}^{\prime} \log g_{\phi}\left(\mathbf{x}_{i}^{\prime}\right)+\left(1-y_{i}^{\prime}\right) \log \left(1-g_{\phi}\left(\mathbf{x}_{i}^{\prime}\right)\right)$
   - 计算example-level fast weights $\phi_{i}^{+}=G_{v}\left(\nabla_{\phi} \mathcal{L}_{i}^{\mathrm{task}}\right)$ 并将其存入'value' memory $M$ 的第 i 个位置
   - 同时使用slow weights和fast weights计算support example的输入表征 $r_{i}^{\prime}=f_{\theta, \theta^{+}}\left(\mathbf{x}_{i}^{\prime}\right)$ 并存入'key' memory $R$ 的第 i 个位置

4. 最后，和往常一样，使用prediction set的样本来计算训练损失

   - 同时使用slow weights和fast weights计算prediction example的输入表征 $r_{j}^{\prime}=f_{\theta, \theta^{+}}\left(\mathbf{x}_{j}\right)$

   - 与上面的MANN类似，通过学习输入表征与记忆矩阵 $R$ 之间的关系，求得读操作的权重：
     $$
     \begin{aligned}
     a_{j} &=\operatorname{cosine}\left(\mathbf{R}, r_{j}\right)=\left[\frac{r_{1}^{\prime} \cdot r_{j}}{\left\|r_{1}^{\prime}\right\| \cdot\left\|r_{j}\right\|}, \ldots, \frac{r_{N}^{\prime} \cdot r_{j}}{\left\|r_{N}^{\prime}\right\| \cdot\left\|r_{j}\right\|}\right] \\
     \phi_{j}^{+} &=\operatorname{softmax}\left(a_{j}\right)^{\top} \mathbf{M}
     \end{aligned}
     $$

   - 更新training loss: $\mathcal{L}_{\text {train }} \leftarrow \mathcal{L}_{\text {train }}+\mathcal{L}^{\text {task }}\left(g_{\phi,\phi^+}\left(\mathbf{x}_{i}\right), y_{i}\right)$

5. 利用 $\mathcal{L}_{train}$ 更新权重 $(\theta, \phi, \omega, \nu)$



## Reference

1. [Meta-Learning: Learning to Learn Fast](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)
2. [🐣 From zero to research — An introduction to Meta-learning](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a)
3. [最前沿：百家争鸣的Meta Learning/Learning to learn](https://zhuanlan.zhihu.com/p/28639662)
4. [论文 | matching net 《Matching Networks for One Shot Learning》](https://www.jianshu.com/p/a87be4be6080)
5. [LSTM-based Meta-Learning 随笔](https://zhuanlan.zhihu.com/p/40522081)
6. [Meta Learning单排小教学](https://zhuanlan.zhihu.com/p/46059552)
7. [Model-Agnostic Meta-Learning （MAML）模型介绍及算法详解](https://zhuanlan.zhihu.com/p/57864886)
8. [Reptile: A Scalable Meta-Learning Algorithm](https://openai.com/blog/reptile/)
9. [Reptile-一阶元学习算法](https://www.cnblogs.com/veagau/p/11816163.html)
10. [Neural Turing Machines](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines) 
11. [Memory Networks](https://arxiv.org/abs/1410.3916)
12. [论文笔记 - Memory Networks 系列](https://zhuanlan.zhihu.com/p/32257642)
13. [元学习论文Meta Learning with MANN翻译及分析](https://zhuanlan.zhihu.com/p/84411963)
14. [Meta-Learning(2)---Memory based方法](https://zhuanlan.zhihu.com/p/61037404)
15. [Meta-Learning论文笔记：Meta Network](https://zhuanlan.zhihu.com/p/66884855)



**十分感谢 lilianweng's blog，让我受益匪浅！**

*Originally published at* [*https://github.com/Skylark0924/Reinforcement-Learning-in-Robotics/*](https://github.com/Skylark0924/Reinforcement-Learning-in-Robotics/blob/master/Related Works/Overcoming Exploration in Reinforcement Learning with Demonstrations.md)*.*