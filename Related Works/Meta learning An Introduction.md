# Meta Learning: An Introduction 

> Meta-learning, also known as â€œlearning to learnâ€, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 
>
> 1) learn an efficient distance metric (metric-based); 
>
> 2) use (recurrent) network with external or internal memory (model-based); 
>
> 3) optimize the model parameters explicitly for fast learning (optimization-based).

Meta learningçš„æ€æƒ³æºäºäººç±»å¯¹è‡ªèº«å­¦ä¹ è¿‡ç¨‹çš„ç†è§£ã€‚ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦feedå¤§é‡çš„æ ·æœ¬ï¼Œå¹¶ä¸”å½“ä»»åŠ¡å‘ç”Ÿæ”¹å˜æ—¶ï¼Œæ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒã€‚ç„¶è€Œå¯¹äºäººç±»æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¿«çš„ç†è§£æ–°çŸ¥è¯†ï¼Œä¾‹å¦‚ä¼šéª‘è‡ªè¡Œè½¦çš„äººå¯ä»¥å¾ˆå¿«ä¸Šæ‰‹æ‘©æ‰˜è½¦ï¼Œç”šè‡³ä¸éœ€è¦ç¤ºèŒƒã€‚Meta learningè¦è§£å†³çš„å°±æ˜¯**å­¦ä¹ ä¸€ä¸ªå¯ä»¥å¿«é€Ÿå­¦ä¹ çš„æ¨¡å‹**ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„'**learning to learn**'ã€‚

ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ— å¤–ä¹å­¦ä¼šåˆ†ç±»ã€å›å½’ç­‰ï¼Œæ˜¯**å­¦ä¼šæ‰§è¡Œä»»åŠ¡**ï¼ˆè¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ï¼‰ï¼Œå¯ä»¥è§†ä½œ**low-level**çš„å­¦ä¹ ã€‚è€Œ Meta learning åˆ™è®¾æƒ³èƒ½å¤Ÿ**æŒæ¡å­¦ä¹ æ–¹æ³•**ï¼Œä½¿å„ç§ä»»åŠ¡éƒ½èƒ½èä¼šè´¯é€šï¼Œè¿™å°±æ˜¯**high-level**çš„å­¦ä¹ äº†ã€‚

å› æ­¤ï¼ŒMeta learning å¹¶ä¸å±€é™äºå“ªä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä¹Ÿä¸å±€é™äºä»»åŠ¡ï¼Œè€Œæ˜¯ä¸€ç§high-level learningçš„æ€æƒ³ï¼Œå› æ­¤å¯ä»¥ç”¨åˆ°supervised learning, reinforcement learningç­‰ä¸­ï¼š

- å³ä½¿ä»…åœ¨æ²¡æœ‰çŒ«çš„å›¾ç‰‡æ•°æ®é›†ä¸­å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥åœ¨è§‚å¯Ÿäº†å°‘æ•°çš„çŒ«ä¹‹åå­¦ä¼šè¯†åˆ«å›¾ä¸­çš„çŒ«
- æ¸¸æˆAIå¯ä»¥å¾ˆå¿«çš„å­¦ä¼šç©æ–°æ¸¸æˆ
- å³ä½¿ä»…åœ¨å¹³å¦çš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œå¾®å‹æœºå™¨äººä¹Ÿå¯ä»¥åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­åœ¨ä¸Šå¡è¡¨é¢å®Œæˆæ‰€éœ€çš„ä»»åŠ¡ã€‚

In this post, I focus on the case when each desired task is a supervised learning problem like image classification. And I will another post for meta-reinforcement learning and its applications in robotics. 

## Definition

### Task as sample

Meta learningä¹‹æ‰€ä»¥å¦‚æ­¤å¼ºå¤§ï¼Œå¾—ç›Šäºå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸€ä¸ªæ•°æ®é›†ä½œä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œå³å°†taskä½œä¸ºsampleã€‚åœ¨ä¸€ä¸ªtaskæ•°æ®é›† $\mathcal{D}=\{(\mathbb{x}_i,y_i)\}$ ä¸­ï¼Œæ•°æ®åˆåˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†: support set $S$ å’Œ prediction set/query set $B$ï¼Œ$\mathcal{D}=\langle S,B\rangle$ã€‚

ä»¥å›¾åƒåˆ†ç±»ä¸ºä¾‹ï¼Œå¯¹äºä¸€ä¸ª *K-shot N-class classification* task æ¥è¯´ï¼Œ$S$ åŒ…å«Nä¸ªç±»åˆ«çš„Kä¸ªæ•°æ®ã€‚

![few-shot-classification](./Meta learning An Introduction.assets/few-shot-classification.png)

é‚£ä¹ˆè¯¥æ€ä¹ˆè®­ç»ƒè¿™ä¸ªæ¨¡å‹å‘¢ï¼Ÿ

1. åœ¨æ ‡ç­¾é›† $\mathcal{L}$ ä¸­å–å­é›† $L$, å°†å¯¹åº”çš„æ•°æ®åˆ†ä¸º $S^L, B^L$ï¼Œè¿™ä¸¤éƒ¨åˆ†æ„æˆ $\mathcal{D}$
2. $S^L$ ä½œä¸º part of model input
3. ç”¨ mini-batch çš„ $B^L$ è®¡ç®— loss and update the model parameters through backpropagation

![image-20191226194740259](./Meta learning An Introduction.assets/image-20191226194740259.png)

ä¸Šå¼ä¸­ï¼Œé»‘è‰²éƒ¨åˆ†æ˜¯supervised learningçš„åŸºæœ¬å…¬å¼ï¼Œçº¢è‰²éƒ¨åˆ†å°±æ˜¯ Meta learning çš„ç‰¹ç‚¹ã€‚ä¸ä¼ ç»Ÿçš„pre-trainedæ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œpre-trainedæ˜¯è¦åœ¨å­ä»»åŠ¡ä¸­è¿›è¡Œfine-tuningï¼Œè€Œ meta learning æ˜¯è¦èƒœä»»å¤§éƒ¨åˆ†çš„ä»»åŠ¡ã€‚

### Learner and Meta-Learner

In addition, another popular view of meta learning decomposes the model update into two stages:

1. ä¸€ä¸ªè¢«ç§°ä½œ'learner'çš„ä¼ ç»Ÿclassifier $f_\theta$ å­¦ä¹ ä¸€ä¸ªç»™å®šçš„task
2. åŒæ—¶ï¼Œä¸€ä¸ªoptimizer $g_\phi$ å­¦ä¹ å¦‚ä½•æ›´æ–°å¦‚ä½•é€šè¿‡ $S$ æ›´æ–° learner çš„å‚æ•°

åœ¨æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åŒæ—¶æ›´æ–° $\theta, \phi$.

![image-20191226200939808](./Meta learning An Introduction.assets/image-20191226200939808.png)

å…³äºè¿™æ–¹é¢çš„ meta learningï¼Œåœ¨ [Ref.2](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a) ä¸­æœ‰å¾ˆè¯¦ç»†çš„è§£é‡Šï¼Œå¹¶ä¸”ä½œè€…å¾ˆç”¨å¿ƒçš„ç»˜åˆ¶äº†ç²¾ç¾çš„åŠ¨å›¾ã€‚

![img](./Meta learning An Introduction.assets/1_AcaPiikZErVv_iFJzWekQg.gif)



### Common Approaches

ä¸åŒçš„ meta learning å®ç°æ–¹å¼ä¸»è¦åŒºåˆ«åœ¨å¦‚ä½•å¯¹ $P_{\theta}(y|\mathbb{x})$ è¿›è¡Œå»ºæ¨¡

![image-20191226202326065](./Meta learning An Introduction.assets/image-20191226202326065.png)



## Metric-Based

ç±»ä¼¼äºæœ€è¿‘é‚»ç®—æ³•ï¼ˆkNNåˆ†ç±»å’Œk-meansèšç±»ï¼‰ä»¥åŠ [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation). The predicted probability over a set of known labels $y$ is a weighted sum of labels of support set samples. The weight is generated by a kernel function $k_\theta$, measuring the similarity between two data samples.
$$
P_{\theta}(y | \mathbf{x}, S)=\sum_{\left(\mathbf{x}_{i}, y_{i}\right) \in S} k_{\theta}\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}
$$
å› æ­¤ï¼Œè¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºå­¦ä¹ ä¸€ä¸ªå¥½çš„**æ ¸å‡½æ•°æ¥åº¦é‡æ ·æœ¬ä¹‹é—´çš„å…³ç³»**ã€‚

ä¸‹é¢ä»‹ç»å‡ ç§**æ ¸å‡½æ•°çš„è®¾è®¡æ–¹æ³•**ã€‚

### Convolutional Siamese Neural Network

 [Siamese neural network](http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf) å°†ä¸¤ä¸ª input åˆ†åˆ«è¿›è¡Œ embed (æ¯”è¾ƒå¥½å¥‡è¿™é‡Œä¸ºä»€ä¹ˆè¦ä½¿ç”¨äº†ä¸¤ä¸ªtwin embed NN)ï¼Œå†æ±‡æ€»åˆ°ä¸€ä¸ªåº¦é‡æ˜ å°„ç½‘ç»œ to judge whether they are in the same class.

![siamese](./Meta learning An Introduction.assets/siamese-conv-net.png)

1. å­¦ä¹  embedding function $f_\theta$
2. ä¸¤ä¸ª embedding çš„L1è·ç¦»ï¼ˆä¹Ÿå¯ä»¥æ˜¯å…¶ä»–çš„åº¦é‡ï¼‰ $L_1=\left|f_{\theta}\left(\mathbf{x}_{i}\right)-f_{\theta}\left(\mathbf{x}_{j}\right)\right|$ 
3. é€šè¿‡ä¸€ä¸ªNNå°†è·ç¦»æ˜ å°„åˆ°ä¸¤ä¸ªå±äºåŒä¸€classçš„æ¦‚ç‡ $p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\sigma\left(\mathbf{W}\left|f_{\theta}\left(\mathbf{x}_{i}\right)-f_{\theta}\left(\mathbf{x}_{j}\right)\right|\right)$
4. ç”±äºlabelæ˜¯äºŒå…ƒçš„ï¼Œå› æ­¤lossé‡‡ç”¨äº¤å‰ç†µ $\mathcal{L}(B)=\sum_{\left(\mathbf{x}_{i}, \mathbf{x}_{i}, y_{i}, y_{j}\right) \in B} \mathbf{1}_{y_{i}=y_{j}} \log p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)+\left(1-\mathbf{1}_{y_{i}=y_{j}}\right) \log \left(1-p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)\right)$
5. å¯¹ç»™å®šçš„ support set $S$ å’Œæµ‹è¯•å›¾ $\mathbb{x}$ (ä¹Ÿå°±æ˜¯prediction set), é¢„æµ‹ç»“æœä¸ºï¼š$\hat{c}_{S}(\mathbf{x})=c\left(\arg \max _{\mathbf{x}_{i} \in S} P\left(\mathbf{x}, \mathbf{x}_{i}\right)\right)$ï¼Œå³å°†ä¸å…¶åº¦é‡æœ€è¿‘çš„support setä¸­çš„æ•°æ®çš„ç±»åˆ«ä½œä¸ºæµ‹è¯•å›¾çš„ç±»åˆ«

è¿™ç§æ–¹æ³•æ˜¯ä¸æ˜¯å¬èµ·æ¥å’Œæœ€è¿‘é‚»ä¹Ÿæ²¡ä»€ä¹ˆåŒºåˆ«ï¼Ÿå…¶å®å…³é”®åœ¨äºå­¦ä¹ å¾—åˆ°çš„embeddingï¼Œæˆ‘ä»¬æœŸæœ›å®ƒå¯ä»¥æ³›åŒ–åˆ°**æœªçŸ¥**çš„ç±»åˆ«ä¸­ï¼Œè¿™å°±æ˜¯å®ƒæ¯”pre-trainedçš„ä¼˜åŠ¿æ‰€åœ¨ã€‚

### Matching Networks

Matching Networksæœ€æ—©ç”± [Vinyals et al., 2016](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf) æå‡º

![siamese](./Meta learning An Introduction.assets/matching-networks.png)
$$
c_{S}(\mathbf{x})=P(y | \mathbf{x}, S)=\sum_{i=1}^{k} a\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}, \text { where } S=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}_{i=1}^{k}
$$
ä¸Siamese Neural Networkä¸åŒï¼ŒMatching Networksä¸­çš„æ ¸å‡½æ•° $a(\mathbb{x}, \mathbb{x}_i)$ é‡‡ç”¨äº†Attention Mechanismå½¢å¼: 
$$
a\left(\mathbf{x}, \mathbf{x}_{i}\right)=\frac{\exp \left(\operatorname{cosine}\left(f(\mathbf{x}), g\left(\mathbf{x}_{i}\right)\right)\right.}{\sum_{j=1}^{k} \exp \left(\operatorname{cosine}\left(f(\mathbf{x}), g\left(\mathbf{x}_{j}\right)\right)\right.}
$$

- cosine è¡¨ç¤ºä¸¤ä¸ªç‰¹å¾çš„cosineè·ç¦»
- $f,g$ ä¸ºå›¾ä¸­çš„ $f_\theta, g_\theta$ ç‰¹å¾æå–å™¨ï¼Œè¿™æ˜¯matching networksçš„**æ ¸å¿ƒéƒ¨åˆ†**

#### Support setç‰¹å¾æå–å™¨ $g_\theta(\mathbb{x}_i, S)$

**æ ¸å¿ƒæ€æƒ³ï¼š**support set ä¸­ $\mathbb{x}$ çš„ç‰¹å¾ä¸æ­¢ä¸å…¶æœ¬èº«æœ‰å…³ï¼Œè¿˜åº”è¯¥ä¸support setä¸­å…¶ä»–çš„æ•°æ®æœ‰å…³

å› æ­¤ï¼Œä½œè€…å°†æ— åºçš„é›†åˆçœ‹æˆæœ‰åºçš„ï¼Œåˆ©ç”¨**åŒå‘LSTM**å¯¹ $g_\theta(\mathbb{x}_i, S)$ è¿›è¡Œå»ºæ¨¡
$$
\begin{aligned}
&\vec{h}_{i}, \vec{c}_{i}=\operatorname{LSTM}\left(g^{\prime}\left(x_{i}\right), \vec{h}_{i-1}, \vec{c}_{i-1}\right)\\
&\overleftarrow{h}_{i}, \overleftarrow{c}_{i}=\operatorname{LSTM}\left(g^{\prime}\left(x_{i}\right), \overleftarrow{h}_{i+1}, \overleftarrow{c}_{i+1}\right)\\
&g\left(x_{i}\right)=\vec{h}_{i}+\overleftarrow{h}_{i}+g^{\prime}\left(x_{i}\right), i=1,2 \ldots k
\end{aligned}
$$

> å…¶ä¸­`g'(x)`, `h(i-1)`, `c(i-1)`, `h(i+1)`, `c(i+1) `åˆ†åˆ«è¡¨ç¤ºåŸå§‹çŠ¶æ€ï¼Œä¸Šä¸€æ—¶åˆ»çš„éšå«çŠ¶æ€ï¼Œä¸Šä¸€æ—¶åˆ»çš„è®°å¿†çŠ¶æ€ï¼Œä¸‹ä¸€æ—¶åˆ»çš„éšå«çŠ¶æ€ï¼Œä¸‹ä¸€æ—¶åˆ»çš„è®°å¿†çŠ¶æ€ï¼›

#### Prediction setç‰¹å¾æå–å™¨ $f_\theta(\mathbb{x}, S)$

é€šè¿‡**æ³¨æ„åŠ›LSTM**è·å–Prediction setçš„ç‰¹å¾
$$
\begin{aligned}
\hat{h}_{k}, c_{k} &=\operatorname{LSTM}\left(f^{\prime}(\hat{x}),\left[h_{k-1}, r_{k-1}\right], c_{k-1}\right) \\
h_{k} &=\hat{h}_{k}+f^{\prime}(\hat{x}) \\
r_{k-1} &=\sum_{i=1}^{|S|} a\left(h_{k-1}, g\left(x_{i}\right)\right) g\left(x_{i}\right) \\
a\left(h_{k-1}, g\left(x_{i}\right)\right) &=e^{h_{k-1}^{T} g\left(x_{i}\right)} / \sum_{j=1}^{|S|} e^{h_{k-1}^{T} g\left(x_{j}\right)}
\end{aligned}
$$

1. Prediction setå…ˆç»è¿‡ä¸€ä¸ªbasic NN (such as CNN)ï¼Œè·å–åŸºæœ¬ç‰¹å¾ $f'(\mathbb{x})$
2. å°†Support setçš„read attention vector $r_{k-1}$ ä½œä¸ºä¸€éƒ¨åˆ†éšå˜é‡è¿›å…¥LSTMè®­ç»ƒ
3. ç»è¿‡Kæ­¥åï¼Œ$f_\theta(\mathbb{x},S)=h_K$

è¿™ç§æ–¹æ³•å«åš'Full Contextual Embeddings (FCE)'ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå®ƒåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°çš„æ¯”ç®€å•ä»»åŠ¡è¦å¥½ã€‚

Matching Networksè¿›ä¸€æ­¥å®Œå–„äº†è®­ç»ƒå’Œæµ‹è¯•çš„æ¡ä»¶åº”è¯¥åŒ¹é…çš„æ€æƒ³ï¼Œè®­ç»ƒè¿‡ç¨‹è¢«è®¾è®¡æˆèƒ½å¤Ÿæ¨¡æ‹Ÿæµ‹è¯•è¿‡ç¨‹çš„æ¨ç†ã€‚

### Relation Network

**Relation Network (RN)** ([Sung et al., 2018](http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf))ç±»ä¼¼äºSiamese Neural Networkï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ä¸åŒï¼š

1. ä½¿ç”¨CNNè®­ç»ƒçš„classifier $g_\phi$ ä»£æ›¿L1è·ç¦»ï¼Œ$r_{ij}=g_\phi([\mathbb{x}_i,\mathbb{x}_j]), [.,.]$ä»£è¡¨concatention
2. Loss functionä½¿ç”¨äº†MSEå–ä»£äº¤å‰ç†µï¼Œè¿™æ˜¯å› ä¸ºRNæ›´æ³¨é‡æ ·æœ¬ä¹‹é—´çš„relationshipï¼Œæ›´ç±»ä¼¼äºå›å½’é—®é¢˜ï¼Œè€Œä¸æ˜¯Siamese Neural Networké‚£æ ·çš„åˆ†ç±»é—®é¢˜

![relation-network](./Meta learning An Introduction.assets/relation-network.png)

### Prototypical Networks

**Prototypical Networks** ([Snell, Swersky & Zemel, 2017](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf)) ä½¿ç”¨äº†**èšç±»**çš„æ€æƒ³ï¼Œå°†æ¯ä¸€ä¸ªinputç¼–ç æˆMç»´ç‰¹å¾å‘é‡ã€‚å¯¹äºåŒä¸€ç±»åˆ«ä¸­çš„æ‰€æœ‰ç‰¹å¾å‘é‡ï¼Œæ±‚å‡ºä¸€ä¸ª *prototype feature* (ä¹Ÿå°±æ˜¯mean vector)

<img src="D:\Github\Reinforcement-Learning-in-Robotics\Related Works\Meta learning An Introduction.assets\prototypical-networks.png" alt="prototypical-networks" style="zoom:67%;" />

Prediction setçš„å±äºå“ªä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡å°±æ˜¯æ ¹æ®å…¶ä¸å„ä¸ªclassä¹‹é—´è·ç¦»çš„å€’æ•°æ¥å®šä¹‰çš„
$$
P(y=c | \mathbf{x})=\operatorname{softmax}\left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c}\right)\right)=\frac{\exp \left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c}\right)\right)}{\sum_{c^{\prime} \in \mathcal{C}} \exp \left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c^{\prime}}\right)\right)}
$$
è‡³äºè·ç¦»çš„åº¦é‡æ–¹æ³•å¯ä»¥è‡ªå·±é€‰å–ï¼Œæ–‡ä¸­é€‰å–çš„æ˜¯æ¬§æ°è·ç¦»ã€‚

Loss functioné‡‡ç”¨ negative log-likelihood: $\mathcal{L}(\theta)=-\log P_{\theta}(y=c | \mathbf{x})$



## Model-Based

Model-Basedä¸Metric-Basedæ–¹æ³•çš„åŒºåˆ«åœ¨äºä¸å¯¹ $P_\theta(y|\mathbb{x})$ çš„å½¢å¼åšå‡è®¾ï¼Œè€Œæ˜¯ä¾é ä¸€ä¸ªå¯ä»¥å¿«é€Ÿå­¦ä¹ çš„modelã€‚modelèƒ½å¤Ÿå¿«é€Ÿæ›´æ–°å‚æ•°ï¼Œå¾—ç›Šäºå…¶å†…éƒ¨ç»“æ„ï¼Œæˆ–ç”±å¦ä¸€ä¸ªå…ƒå­¦ä¹ å™¨æ¨¡å‹æ¥æ§åˆ¶ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒModel-Basedæ–¹æ³•æ˜¯åœ¨å¯»æ‰¾æœ€ä¼˜æ¶æ„ã€‚

### Memory-Augmented Neural Networks(MANN)



## Optimization-Based 

æˆ‘ä»¬çŸ¥é“ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ ç½‘ç»œå‚æ•°çš„æ›´æ–°ï¼Œéƒ½æ˜¯é€šè¿‡gradient backpropagationå®ç°çš„ã€‚ä½†æ˜¯è¿™ç§æ–¹å¼å¹¶ä¸é€‚ç”¨äºå°‘æ ·æœ¬çš„æƒ…å†µï¼ˆæ ·æœ¬å°‘ï¼Œæ¢¯åº¦æ–¹å‘æ— æ³•è¡¨å¾æ€»ä½“æ¢¯åº¦æ–¹å‘ï¼Œæ˜“å‘æ•£ï¼‰ã€‚å› æ­¤è‹¥æƒ³åœ¨å°‘æ ·æœ¬çš„æƒ…å†µä¸‹ä¾ç„¶èƒ½å¤Ÿå¾ˆå¥½çš„å­¦ä¹ ï¼Œå°±éœ€è¦åœ¨optimizationæ–¹å¼ä¸Šè¿›è¡Œæ”¹å˜ã€‚è¿™å°±æ˜¯Optimization-Based meta-learning(å°±æ˜¯ä¸Šæ–‡æ‰€è¯´çš„Learner and Meta-Learner)çš„æ€è·¯ã€‚

### LSTM Meta-Learner

- Pytorch Code: https://github.com/markdtw/meta-learning-lstm-pytorch

ä¸åŒäºä¼ ç»Ÿçš„å…¬å¼åŒ–æ›´æ–°ï¼ŒOptimization ä¹Ÿå¯ä»¥ä½œä¸ºModelè¿›è¡Œç²¾ç»†åŒ–å»ºæ¨¡ï¼Œ[Ravi & Larochelle (2017)](https://openreview.net/pdf?id=rJY0-Kcll)å°†optimization modelå‘½åä¸º 'Meta-learner'ï¼Œè€ŒåŸæ¥çš„modelå«åš 'Learner'ã€‚

Letâ€™s denote the learner model as $M_\theta$ parameterized by $\theta$, the meta-learner as $R_\Theta$ with parameters $\Theta$, and the loss function $\mathcal{L}$.

#### ä¸ºä»€ä¹ˆæ˜¯LSTMï¼Ÿ

1. ä½œè€…å‘ç°LSTMçš„æ›´æ–°è§„åˆ™å’Œä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´æ–°è§„åˆ™éå¸¸ç±»ä¼¼
2. çŸ¥é“æ¢¯åº¦çš„å†å²æœ‰åˆ©äºæ¢¯åº¦çš„æ›´æ–°ï¼Œå°±åƒåŠ¨é‡ä¸€æ ·

#### ç®—æ³•æ€è·¯

**gradient decent**å¯¹Learnerçš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š
$$
\theta_{t}=\theta_{t-1}-\alpha_{t} \nabla_{\theta_{t-1}} \mathcal{L}_{t}
$$
**LSTMä¸­å•å…ƒçŠ¶æ€ï¼ˆcell stateï¼‰**çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š
$$
\begin{equation}\label{6}
c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t}
\end{equation}
$$
å¦‚æœLSTMä¸­çš„é—å¿˜é—¨ $f_t=1$ï¼Œè¾“å…¥é—¨ $i_t=\alpha_t$ï¼Œcell state $c_t=\theta_t$ï¼Œnew cell state $\tilde{c}_{t}=-\nabla_{\theta_{t-1}} \mathcal{L}_{t}$. é‚£ä¹ˆ Equation $\ref{6}$ å°±å¯ä»¥**è½¬å˜ä¸ºæ¢¯åº¦æ›´æ–°å½¢å¼**ï¼š
$$
\begin{aligned}
c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\
&=\theta_{t-1}-\alpha_{t} \nabla_{\theta_{t-1}} \mathcal{L}_{t}
\end{aligned}
$$
æ˜¯ä¸æ˜¯æ„Ÿè§‰ideaçš„äº§ç”Ÿå°±æ˜¯å¦‚æ­¤çš„ç®€å•ï¼

å½“ç„¶ï¼Œæœ‰äº†ideaè¿˜éœ€è¦å®Œå–„ã€‚ç›´æ¥è®©é—å¿˜é—¨å’Œè¾“å…¥é—¨å–å®šå€¼ä¼šè®©LSTMå¤±å»äº†å¾ˆå¤šå¨åŠ›ï¼Œå› æ­¤ä½œè€…è¿›è¡Œäº†refine:
$$
\begin{aligned}
&f_{t}=\sigma\left(\mathbf{W}_{f} \cdot\left[\nabla_{\theta_{t-1}} \mathcal{L}_{t}, \mathcal{L}_{t}, \theta_{t-1}, f_{t-1}\right]+\mathbf{b}_{f}\right) \\ &\qquad\text {  how much to forget the old value of parameters   } \theta_{t-1}\\
&i_{t}=\sigma\left(\mathbf{W}_{i} \cdot\left[\nabla_{\theta_{t}, 1} \mathcal{L}_{t}, \mathcal{L}_{t}, \theta_{t-1}, i_{t-1}\right]+\mathbf{b}_{i}\right) \\ &\qquad\text {  corresponding to the learning rate at time step t. }\\
&\tilde{\theta}_{t}=-\nabla_{\theta_{t-1}} \mathcal{L}_{t}\\
&\theta_{t}=f_{t} \odot \theta_{t-1}+i_{t} \odot \tilde{\theta}_{t}
\end{aligned}
$$

#### æ¨¡å‹å»ºç«‹

![lstm-meta-learner](./Meta learning An Introduction.assets/lstm-meta-learner.png)

ä¸ä¹‹å‰ä»‹ç»çš„Matching Networksä¸€æ ·ï¼ŒLSTM Meta-Learnerä¹Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿæµ‹è¯•ç¯èŠ‚ã€‚

1. Sample a training dataset $\mathcal{D}=\left(\mathcal{D}_{\text {support}}, \mathcal{D}_{\text {pred}}\right) \in \hat{\mathcal{D}}_{\text {meta-train }}$
2. ç”¨ $\mathcal{D}_{\text {support }}$æ›´æ–°Tè½® Learner å‚æ•° $\theta$ 
3. æœ€åä¸€ä¸ªcell stateï¼Œä¹Ÿå°±æ˜¯ $\theta_T$ è¢«ç”¨æ¥åœ¨ $\mathcal{D}_{\text {pred}}$ ä¸Šè®­ç»ƒ Meta-Learner $ \Theta$

![train-meta-learner](./Meta learning An Introduction.assets/train-meta-learner.png)

#### Tricks

ç”±äºLSTMä¸€èˆ¬æ˜¯ç”¨äºæ·±åº¦ç½‘ç»œçš„ï¼Œå‚æ•°ä¼—å¤šã€‚ä¸ºé¿å…å‚æ•°çˆ†ç‚¸ï¼Œä½œè€…åšäº†**å‚æ•°å…±äº«**ã€‚å…·ä½“æ€è·¯æ˜¯ sharing parameters across coordinatesï¼Œè¯¦è§ [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)ã€‚**ç®€è€Œè¨€ä¹‹ï¼Œå¯¹äºåŒä¸€å±‚ä¸­çš„å‚æ•° $\theta_i$ï¼Œä»–ä»¬çš„æ›´æ–°è§„åˆ™ç›¸åŒï¼Œå³æƒé‡å’Œåå·® $W,b$ ç›¸åŒã€‚** 



### Model-Agnostic Meta-Learning(MAML)

- Pytorch Codeï¼š[dragen1860/MAML-Pytorch](https://link.zhihu.com/?target=https%3A//github.com/dragen1860/MAML-Pytorch) æˆ– [katerakelly/pytorch-maml](https://link.zhihu.com/?target=https%3A//github.com/katerakelly/pytorch-maml)
- Tensorflow Codeï¼š[cbfinn/maml](https://link.zhihu.com/?target=https%3A//github.com/cbfinn/maml)

Berkeleyçš„ Finn åœ¨meta-learningé¢†åŸŸåˆ›é€ äº†å¾ˆå¤šå¼€åˆ›æ€§çš„æˆæœï¼Œå°¤å…¶æ˜¯åœ¨meta-reinforcement learning in roboticsæ–¹é¢ï¼Œä»¥åä¼šè¯¦ç»†è®²è§£è¿™æ–¹é¢çš„æˆæœã€‚

**Model-Agnostic Meta-Learning** ([Finn, et al. 2017](https://arxiv.org/abs/1703.03400)) æ˜¯ä¸€ä¸ªéå¸¸é€šç”¨çš„optimization algorithmï¼Œå®ƒå¯ä»¥é™„åŠ åœ¨ä»»ä½•ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸Šï¼Œæ‰€ä»¥å«åš**model-agnostic**å³**æ¨¡å‹æ— å…³**ã€‚

#### ç®—æ³•æ€è·¯

å‡è®¾modelä¸º $f_\theta$ï¼Œç»™å®štask $\tau_i$ ä»¥åŠæ•°æ®é›† $(\mathcal{D}_{support}^{(i)}, \mathcal{D}_{pred}^{(i)})$ï¼Œæˆ‘ä»¬ä»¥å¾€çš„æ›´æ–°æ–¹å¼æ˜¯ï¼š
$$
\begin{equation}\label{7}
\theta_{i}^{\prime}=\theta-\alpha \nabla_{\theta} \mathcal{L}_{\tau_{i}}^{(0)}\left(f_{\theta}\right)
\end{equation}
$$
è¿™é‡Œ $\mathcal{L}^{(0)}$ ä»£è¡¨idä¸º0çš„mini data batchä¸Šçš„lossã€‚

ä½†æ˜¯ Equation $\ref{7}$ ä»…ä»£è¡¨ä¸€ä¸ªtaskä¸Šçš„ä¼˜åŒ–ï¼Œè¦æƒ³æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„tasksä¸Šå…·æœ‰æ³›åŒ–æ€§ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æœ€ä½³çš„ $\theta^*$ã€‚æˆ‘ä»¬ç»§ç»­sampleä¸€ä¸ªæ–°çš„data batch id(1)ï¼Œå®ƒçš„losså°±æ˜¯ $\mathcal{L}^{(1)}$ï¼Œé‚£ä¹ˆæœ€ä¼˜çš„ $\theta^* $ å°±æ˜¯ï¼š
$$
\begin{aligned}
\theta^{*} &=\arg \min _{\theta} \sum_{\tau_{i} \sim p(\tau)} \mathcal{L}_{\tau_{i}}^{(1)}\left(f_{\theta_{i}^{\prime}}\right)=\arg \min _{\theta} \sum_{\tau_{i} \sim p(\tau)} \mathcal{L}_{\tau_{i}}^{(1)}\left(f_{\theta-\alpha \nabla_{\theta} \mathcal{L}_{\tau_i}^{(0)}\left(f_{\theta}\right)}\right) \\
\theta & \leftarrow \theta-\beta \nabla_{\theta} \sum_{\tau_{i} \sim p(\tau)} \mathcal{L}_{\tau_{i}}^{(1)}\left(f_{\theta-\alpha \nabla_{\theta} \mathcal{L}_{\tau_{i}}^{(0)}\left(f_{\theta}\right)}\right)
\end{aligned}
$$
<img src="./Meta learning An Introduction.assets/maml-algo.png" alt="MAML Algorithm" style="zoom: 50%;" />

**è§£é‡Šä¸€ä¸‹ï¼š** 

1. $\mathcal{L}_{\tau_i}^{(j)}$ ä¸‹æ ‡æ˜¯ä»»åŠ¡ $\tau_i\sim p(\tau)$, ä¸Šæ ‡æ˜¯æŒ‡åœ¨ä»»åŠ¡ $\tau_i $ ä¸‹sampleçš„data batch id(j)ã€‚
2. $\theta_i'$ ä»£è¡¨åœ¨task $\tau_i$ ä¸‹åˆ©ç”¨ $\mathcal{D}_{support}$ è¿›è¡Œ**ç¬¬ä¸€æ¬¡æ¢¯åº¦æ›´æ–°**å¾—åˆ°çš„**å‚æ•°ä¸­é—´ç»“æœ**ï¼Œå¹¶**ä¸ç›´æ¥ä½œç”¨**äºmodel $f_\theta $çš„æ›´æ–°ã€‚
3. åœ¨å¾—åˆ°å„ä¸ªtaskçš„å‚æ•°ä¸­é—´ç»“æœä¹‹åï¼Œåœ¨ä¸­é—´ç»“æœçš„åŸºç¡€ä¸Šåˆ©ç”¨ $\mathcal{D}_{pred}$ å†è¿›è¡Œ**ç¬¬äºŒæ¬¡æ¢¯åº¦æ›´æ–°**ï¼Œè¿™æ¬¡**ç›´æ¥ä½œç”¨**äºmodelå‚æ•°çš„æ›´æ–°ã€‚å¯ä»¥å°†è¿™æ¬¡æ¢¯åº¦æ›´æ–°çš„æ–¹å‘ç†è§£ä¸ºå„ä¸ªtask**ç»¼åˆåšå¼ˆçš„æœ€ä¼˜æ–¹å‘**ã€‚

ä¸‹å›¾å±•ç¤ºçš„å°±æ˜¯åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šçš„**æ¢¯åº¦å­æ–¹å‘**ï¼Œå¤§çš„é»‘è‰²ç®­å¤´å°±æ˜¯åšå¼ˆåçš„**æ¢¯åº¦ä¸»æ–¹å‘**ï¼Œä»¥ä¸Šéƒ½æ˜¯model **pre-trained**è¿‡ç¨‹ã€‚å½“æˆ‘ä»¬éœ€è¦å¯¹æŸä¸ªtask **fine-tuning**çš„æ—¶å€™ï¼Œå¦‚å›¾ä¸­çš„è™šçº¿ï¼Œåœ¨pre-trainedçš„å‚æ•°åŸºç¡€ä¸Šï¼Œç»è¿‡few-shotå°±å¯ä»¥è¾¾åˆ°ç›¸åº”çš„æœ€ä¼˜å‚æ•°ã€‚

<img src="./Meta learning An Introduction.assets/maml.png" alt="MAML" style="zoom:33%;" />

#### Tricks

ç”±äºMAMLçš„è®¡ç®—ç”¨åˆ°äº†**äºŒæ¬¡æ¢¯åº¦**ï¼Œä¸ºé¿å…æé«˜è®¡ç®—å¤æ‚åº¦ï¼Œè¯ç”Ÿäº†MAMLçš„ä¿®æ”¹ç‰ˆï¼š**First-Order MAML (FOMAML)**ã€‚

å‡è®¾æˆ‘ä»¬åœ¨ç¬¬ä¸€æ¬¡æ¢¯åº¦æ›´æ–°è¿›è¡Œ k æ­¥ï¼Œä»åˆå§‹å‚æ•° $\theta_{meta}$ å¼€å§‹ï¼š
$$
\begin{aligned}
\theta_{0}&=\theta_{\text {meta }}\\
\theta_{1}&=\theta_{0}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{0}\right)\\
\theta_{2}&=\theta_{1}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{1}\right)\\
&...\\
\theta_{k}&=\theta_{k-1}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{k-1}\right)
\end{aligned}
$$
åœ¨ç¬¬äºŒæ¬¡æ›´æ–°çš„æ—¶å€™å°±æ˜¯ï¼š
$$
\theta_{\mathrm{meta}} \leftarrow \theta_{\mathrm{meta}}-\beta g_{\mathrm{MAML}}
$$
å…¶ä¸­
$$
\begin{aligned}
g_{\mathrm{MAML}} &=\nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{k}\right) \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot\left(\nabla_{\theta_{k-1}} \theta_{k}\right) \ldots\left(\nabla_{\theta_{0}} \theta_{1}\right) \cdot\left(\nabla_{\theta} \theta_{0}\right) \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k} \nabla_{\theta_{-1}} \theta_{i} \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k} \nabla_{\theta_{-1}}\left(\theta_{i-1}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{i-1}\right)\right) \\
&=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k}\left(I-\alpha \nabla_{\theta_{i-1}}\left(\nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{i-1}\right)\right)\right)
\end{aligned}
$$
MAMLçš„æ›´æ–°å°±æ˜¯ä¸Šå¼ï¼š
$$
\begin{equation}\label{9}
g_{\mathrm{MAML}}=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right) \cdot \prod_{i=1}^{k}\left(I-\alpha \nabla_{\theta_{i-1}}\left(\nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{i-1}\right)\right)\right)
\end{equation}
$$
FOMAMLç›´æ¥å¿½ç•¥äº† Equation $\ref{9}$ ä¸­çš„äºŒæ¬¡æ¢¯åº¦ï¼š
$$
g_{\mathrm{FOMAML}}=\nabla_{\theta_{k}} \mathcal{L}^{(1)}\left(\theta_{k}\right)
$$
å¯¹ï¼Œå°±æ˜¯è¿™ä¹ˆç®€å•ç²—æš´ï¼Œéš¾ç®—çš„æˆ‘ä»¬å°±ä¸è¦äº†...



### Reptile

- Tensorflow Code: https://github.com/openai/supervised-reptile

**Reptile** ([Nichol, Achiam & Schulman, 2018](https://arxiv.org/abs/1803.02999)) æ˜¯ä¸€ç§éå¸¸ç®€å•çš„optimization algorithmã€‚å®ƒå’ŒMAMLä¸€æ ·æ˜¯åŸºäºæ¢¯åº¦çš„å¹¶ä¸”ä¹Ÿæ˜¯model-agnosticã€‚è¿™æ˜¯OpenAIçš„ä¸€é¡¹å·¥ä½œï¼Œå¯ä»¥è¯»ä¸€ä¸‹[OpenAI blog](https://openai.com/blog/reptile/)

#### ç®—æ³•æ€è·¯

ç”±äºMAMLå…·æœ‰äºŒæ¬¡æ¢¯åº¦ï¼ŒReptileè®¾æ³•åœ¨**ç¬¬äºŒæ¬¡æ¢¯åº¦æ›´æ–°**æ—¶æŠ›å¼ƒæ¢¯åº¦çš„è®¡ç®—ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç§æ›´softçš„æ–¹å¼å¾—åˆ°**ç»¼åˆåšå¼ˆæ–¹å‘**ï¼š
$$
\theta \leftarrow \theta+\alpha \frac{1}{n} \sum_{i=1}^{n}\left(W_{i}-\theta\right)
$$
<img src="D:\Github\Reinforcement-Learning-in-Robotics\Related Works\Meta learning An Introduction.assets\reptile-algo.png" alt="Reptile Algorithm" style="zoom: 25%;" />

#### Tricks

Reptileä¹ä¸€çœ‹å’Œæ™®é€šçš„SGDæ²¡ä»€ä¹ˆåŒºåˆ«ã€‚å½“SGDçš„æ¢¯åº¦æ›´æ–°step k=1æ—¶ï¼Œreptileå…¶å®å°±æ˜¯ç›¸å½“äºåœ¨**å…¨é›†**ä¸Šè¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œä½†æ˜¯è¿™æ ·çš„ 'joint training' å¯èƒ½ä¼šå¯¼è‡´æŸäº›ç‰¹å®šçš„taskå¾—ä¸åˆ°å…³æ³¨ã€‚ä¾‹å¦‚åœ¨zero-shoté—®é¢˜ä¸­ï¼Œè¿™æ ·åšå°±æ”¶æ•ˆç”šå¾®ã€‚å› æ­¤éœ€è¦ k>1ï¼Œä½¿å…¶åŒ…å«äºŒé˜¶æˆ–æ›´é«˜é˜¶å¾®åˆ†é¡¹ã€‚

æ›´å¤šçš„ç†è®ºåˆ†æè¯¦è§ [Ref. 9](https://www.cnblogs.com/veagau/p/11816163.html) å’Œ [Ref. 1]([Ref. 9])

#### Reptile vs FOMAML

å‡è®¾**ç¬¬ä¸€æ¬¡æ¢¯åº¦æ›´æ–°**è¿›è¡Œä¸¤æ­¥ï¼Œå³ k=2ï¼ŒåƒMAMLé‡Œä¸€æ ·ï¼Œå®ƒä»¬çš„æŸå¤±ä¸º $\mathcal{L}^{(0)}, \mathcal{L}^{(1)}$

è®¾ $g_{j}^{(i)}=\nabla_{\theta} \mathcal{L}^{(i)}\left(\theta_{j}\right)$, $H_{j}^{(i)}=\nabla_{\theta}^{2} \mathcal{L}^{(i)}\left(\theta_{j}\right)$
$$
\begin{equation}
\begin{aligned}
&\theta_{0}=\theta_{\text {meta }}\\
&\theta_{1}=\theta_{0}-\alpha \nabla_{\theta} \mathcal{L}^{(0)}\left(\theta_{0}\right)=\theta_{0}-\alpha g_{0}^{(0)}\\
&\theta_{2}=\theta_{1}-\alpha \nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{1}\right)=\theta_{0}-\alpha g_{0}^{(0)}-\alpha g_{1}^{(1)}
\end{aligned}
\end{equation}\label{12}
$$
é‚£ä¹ˆMAML, FOMAML, Reptileçš„**ç¬¬äºŒæ¬¡æ›´æ–°**å¯ä»¥å†™ä½œï¼š
$$
\begin{equation}
\begin{aligned}
g_{\mathrm{FOMAML}} &=\nabla_{\theta_{1}} \mathcal{L}^{(1)}\left(\theta_{1}\right)=g_{1}^{(1)} \\
g_{\mathrm{MAML}} &=\nabla_{\theta_{1}} \mathcal{L}^{(1)}\left(\theta_{1}\right) \cdot\left(I-\alpha \nabla_{\theta}^{2} \mathcal{L}^{(0)}\left(\theta_{0}\right)\right)=g_{1}^{(1)}-\alpha H_{0}^{(0)} g_{1}^{(1)}\\
g_{\text {Reptile }}&=\left(\theta_{0}-\theta_{2}\right) / \alpha=g_{0}^{(0)}+g_{1}^{(1)}
\end{aligned}
\end{equation}\label{13}
$$
<img src="./Meta learning An Introduction.assets/reptile_vs_FOMAML.png" alt="Reptile vs FOMAML" style="zoom: 20%;" />

å°† Equation $\ref{13}$ è¿›è¡Œæ³°å‹’å±•å¼€ï¼Œç”±äº $\theta_{1}-\theta_{0}=-\alpha g_{0}^{(0)}$
$$
\begin{aligned}
g_{1}^{(1)} &=\nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{1}\right) \\
&=\nabla_{\theta} \mathcal{L}^{(1)}\left(\theta_{0}\right)+\nabla_{\theta}^{2} \mathcal{L}^{(1)}\left(\theta_{0}\right)\left(\theta_{1}-\theta_{0}\right)+\frac{1}{2} \nabla_{\theta}^{3} \mathcal{L}^{(1)}\left(\theta_{0}\right)\left(\theta_{1}-\theta_{0}\right)^{2}+\ldots \\
&=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+\frac{\alpha^{2}}{2} \nabla_{\theta}^{3} \mathcal{L}^{(1)}\left(\theta_{0}\right)\left(g_{0}^{(0)}\right)^{2}+\ldots \\
&=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+O\left(\alpha^{2}\right)
\end{aligned}
$$
å› æ­¤Equation $\ref{13}$å¯ä»¥å†™ä½œï¼š
$$
\begin{aligned}
g_{\mathrm{FOMAML}} &=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+O\left(\alpha^{2}\right) \\
g_{\mathrm{MAML}}&=g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}-\alpha H_{0}^{(0)} g_{0}^{(1)}+O\left(\alpha^{2}\right)\\
g_{\text {Reptile }} &=g_{0}^{(0)}+g_{0}^{(1)}-\alpha H_{0}^{(1)} g_{0}^{(0)}+O\left(\alpha^{2}\right)
\end{aligned}
$$

- $A=\mathbb{E}_{\tau, 0,1}\left[g_{0}^{(0)}\right]=\mathbb{E}_{\tau, 0,1}\left[g_{0}^{(1)}\right]$ ä»£è¡¨å„ä¸ªtasks lossçš„å¹³å‡æ¢¯åº¦ã€‚Aç”¨äº**æé«˜taskå±‚æ¬¡çš„æ€§èƒ½**ï¼Œåˆå¯ç§°ä½œ AvgGradï¼›
- $B=\mathbb{E}_{\tau, 0,1}\left[H_{0}^{(1)} g_{0}^{(0)}\right]=\frac{1}{2} \mathbb{E}_{\tau, 0,1}\left[H_{0}^{(1)} g_{0}^{(0)}+H_{0}^{(0)} g_{0}^{(1)}\right]=\frac{1}{2} \mathbb{E}_{\tau, 0,1}\left[\nabla_{\theta}\left(g_{0}^{(0)} g_{0}^{(1)}\right)\right]$ ä»£è¡¨æ¯ä¸€ä¸ªtaskå†…ä¸¤ä¸ªmini-batchçš„æ¢¯åº¦æ–¹å‘çš„å†…ç§¯ã€‚Bç”¨äº**æé«˜dataå±‚æ¬¡çš„æ€§èƒ½**ï¼Œåˆå¯ç§°ä½œ AvgGradInnerã€‚

$$
\begin{array}{l}
{\mathbb{E}_{\tau, 1,2}\left[g_{\text {FOMAML }}\right]=A-\alpha B+O\left(\alpha^{2}\right)} \\
{\mathbb{E}_{\tau, 1,2}\left[g_{\text {MAML }}\right]=A-2 \alpha B+O\left(\alpha^{2}\right)} \\
{\mathbb{E}_{\tau, 1,2}[{g_{ \text {Reptili} }}]=2 A-\alpha B+O\left(\alpha^{2}\right)}
\end{array}
$$

è¿™æ ·çœ‹ä¸‰ç§æ–¹æ³•çš„ä¾§é‡ç‚¹å°±å¾ˆæ˜æ˜¾äº†ã€‚å¯ä»¥çœ‹åˆ°ä¸‰è€…AvgGradInnerä¸AvgGradä¹‹é—´çš„ç³»æ•°æ¯”çš„å…³ç³»æ˜¯ï¼š**MAML > FOMAML > Reptile**ã€‚



## Reference

1. [Meta-Learning: Learning to Learn Fast](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)
2. [ğŸ£ From zero to research â€” An introduction to Meta-learning](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a)
3. [æœ€å‰æ²¿ï¼šç™¾å®¶äº‰é¸£çš„Meta Learning/Learning to learn](https://zhuanlan.zhihu.com/p/28639662)
4. [è®ºæ–‡ | matching net ã€ŠMatching Networks for One Shot Learningã€‹](https://www.jianshu.com/p/a87be4be6080)
5. [LSTM-based Meta-Learning éšç¬”](https://zhuanlan.zhihu.com/p/40522081)
6. [Meta Learningå•æ’å°æ•™å­¦](https://zhuanlan.zhihu.com/p/46059552)
7. [Model-Agnostic Meta-Learning ï¼ˆMAMLï¼‰æ¨¡å‹ä»‹ç»åŠç®—æ³•è¯¦è§£](https://zhuanlan.zhihu.com/p/57864886)
8. [Reptile: A Scalable Meta-Learning Algorithm](https://openai.com/blog/reptile/)
9. [Reptile-ä¸€é˜¶å…ƒå­¦ä¹ ç®—æ³•](https://www.cnblogs.com/veagau/p/11816163.html)



*Originally published at* [*https://github.com/Skylark0924/Reinforcement-Learning-in-Robotics/*](https://github.com/Skylark0924/Reinforcement-Learning-in-Robotics/blob/master/Related Works/Overcoming Exploration in Reinforcement Learning with Demonstrations.md)*.*