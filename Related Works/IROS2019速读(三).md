# IROS2019速读(三)

## Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment

### Keywords

affordance map, robotic hands design, suction cup, 

### Main Idea

Base Model = Affordance ConvNet + U-Net + DQN

设计了一款吸盘和抓手结合的机械手，使用affordance map来评估每个像素的confidence rate for grasping，使用DQN来使机器人主动探索，调整环境，以使affordance map更适宜抓取。

![image-20191221113203207](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(三).assets\image-20191221113203207.png)

（吐槽一下：Tsinghua写文章都不贴矢量图的？？？）

![image-20191221114139768](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(三).assets\image-20191221114139768.png)

DQN结构放大图：

![image-20191221114316092](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(三).assets\image-20191221114316092.png)

本文设计了八个动作，机器人可以向八个方向让物体移动固定距离，可以想象这必然是一个model-based的方法，要不然也不会使用离散的DQN算法。使用[U-Net](https://zhuanlan.zhihu.com/p/43927696)(一种用于语义分割的卷积网络)作为基础网络，8个相同的tiny U-Net对应8个动作，最后输出8个confidence scores。这个action具体怎么选也没细说，不过猜测也是argmax、$\epsilon$-greedy 这种正常操作。

奖励函数设置：

- 是否适宜抓取使用Affordence map的形状与高斯分布的近似程度来判断，得到 flat metric $\Phi_f$
- 判断图像中是否有多个峰值，以峰值之间距离判定，得到 interval metric $\Phi_d$
- Affordence map 最大confidence value $\nu_M$

总的奖励函数如下：
$$
\Phi=\lambda_f*\Phi_f+\lambda_d*\Phi_d+\lambda_\nu*\nu_M
$$

### Opinion

思想很简单，主要是设计了一种抓手，DQN只是辅助。

文中提到该模型有两个缺点：

1. 所设计的奖励函数不能区分出所有坏的场景
2. 有的时候在没有物体的地方输出无用的动作



## A Comparison of Action Spaces for Learning Manipulation Tasks

### Keywords

action space

### Main Idea

本文主要对比了不同的action选择对manipulation的影响，算是一篇综述文，action chosen包括以下几种：

- torque
- joint 
- inverse dynamic
- impedance control

同时，评估了两种连续控制RL：

- PPO
- DDPG

实验：

- Hammer
- Insertion
- Pushing

结论：

![image-20191221200948469](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(三).assets\image-20191221200948469.png)

总而言之，阻抗控制器最好。其最大的特点就是动作在任务空间而不在位形空间。

### Opinion

本文仅在simulator中做实验，但是总结的经验还是可以在实验中参考的。



## Pixel-Attentive Policy Gradient for Multi-Fingered Grasping in Cluttered Scenes

Paper: http://arxiv.org/abs/1903.03227

Video: http://crlab.cs.columbia.edu/pixelattentivegrasping

### Keywords

on-policy, sim-to-real, depth image, attention mechanism

### Main Idea

![image-20191221234256181](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(三).assets\image-20191221234256181.png)

引入Attention mechanism，使机器人聚焦在细节上。即先连续zooming到合适的位置，再进行抓取。

本文的重点在于high-dimension actions，包括以下十种：

1. $a_t^{position}$ 代表抓取时的末端位置以及zooming过程中机器人注意区域的中心位置
2. $a_t^{zoom}$ 代表是zooming还是grasping， $a_t^{scale}$代表放缩的比例
3. $a_t^{roll}, a_t^{pitch}, a_t^{yaw}$ 代表抓取时末端的航向角
4. $a_t^{spread}, a_t^{finger1}, a_t^{finger2}, a_t^{finger3}$ 代表抓取时 4-DOF Barrett 侧向速度以及三个手指的角度

以上的actions都是使用policy gradient来判断
$$
\begin{array}{l}{\log \pi_{\theta}\left(a_{t} | s_{t}\right)} \\ {=\log \pi_{\theta}\left(a_{t}^{\text {position}} | s_{t}\right)+\log \pi_{\theta}\left(a_{t}^{z o o m} | s_{t}, a_{t}^{\text {position}}\right)} \\ {+a_{t}^{z o o m} \times \log \pi_{\theta}\left(a_{t}^{\text {scale}} | s_{t}, a_{t}^{\text {position}}\right)} \\ {+\left(1-a_{t}^{z o o m}\right) \times \sum_{d o f \in D O F s} \log \pi_{\theta}\left(a_{t}^{\text {dof}} | s_{t}, a_{t}^{\text {position}}\right)}\end{array}
$$
策略梯度颇为复杂。文中使用的是Clipped PPO。

### Opinion

这篇文章的想法是让机器人在脑海中反复挑选该抓的位置，而不是直接开始尝试抓取。