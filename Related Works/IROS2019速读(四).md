# IROS2019 Paper速读(四)

## Cascaded Gaussian Processes for Data-efficient Robot Dynamics Learning

Paper: http://arxiv.org/abs/1910.02291

### Keywords

cascaded Gaussian processes, inverse dynamics learning

### Main Idea

提出了一种级联高斯过程用于机器人逆运动学的学习，通过使用在一个关节上训练的GP的输出来替代所有先前的关节状态，来递归地减小回归问题的尺寸，从而减小特征空间的尺寸。级联GP由递归NE(Newton-Euler)方程启发.

![image-20191222112555208](./IROS2019速读(四).assets/image-20191222112555208.png)

Standard GP: 使用所有关节的states进行训练

Cascaded GP: 用关节转矩表征关节states

由于使用欧拉运动公式需要每个关节都有力/力矩传感器，所以本文假设使用关节扭矩就可以在某种程度上总结外部关节的影响。

Cascaded GP的inward recursion，相应GP预测出的关节转矩 $\tau_{i+1}$ 会被用于代替外部关节( $i+1\sim N$) 的states

outward recursion则从机器人基座开始，以同样的道理代替内部关节($1\sim i+1$)的states，从而降低维度。

除此之外，还是用 Derivative-Free Features 作为解决数值微分嘈杂本质的一种手段，这是获得联合速度和加速度的必然步骤。


### Opinion

文中建议考虑使用semi-parametric, derivative-based feature结合cascaded approach，这样可以提高模型的generalization。由于我对这个方向不太熟，就不做过多评论了。



## Learning Real-World Robot Policies by Dreaming

Paper: http://arxiv.org/abs/1805.07813

Website: https://piergiaj.github.io/robot-dreaming-policy/

### Keywords

data efficiency, real-world, dreaming model(world model)

### Main Idea

设计了一个dreaming model，使机器人在其中进行interaction，而不是直接和real-world。这个思想和上上期的Learning Physics-Based Manipulation in Clutter: Combining Image-Based Generalization and Look-Ahead Planning有些相似。

Dreaming Model 由 FCNN, VAE, action-conditioned future regressor(ACFR)构成。

ACFR: 可以模拟机器人执行指令action之后的state变化。这就意味着，Dreaming Model相较于之前的Model-based方法，引入了 imagined trajectories 来代替之前的 real trajectories，这也是作者 use the word 'dreaming' rather than 'model-based' 的用意。详见reddit上的[debate](https://www.reddit.com/r/reinforcementlearning/comments/8rmcxn/learning_realworld_robot_policies_by_dreaming/).

以下是dreaming生成的imagine trajectories的可视化：

![](./IROS2019速读(四).assets/dream_2.gif)

![](./IROS2019速读(四).assets/dream_6.gif)

It is really awesome, isn't it?

那我们接下来看一下如此marvelous的dreaming是如何实现的吧！
$$
\begin{array}{c}{\text { State Encoder } E n c_{\theta}: I \mapsto s} \\ {\text { State Decoder } D e c_{\theta}: s \mapsto I} \\ {\text { State-transition } f_{\theta}: s_{t}, a_{t} \mapsto s_{t+1}} \\ {\text { Reward/End Detector } R_{\theta}: s \mapsto r}\end{array}
$$
![image-20191222123748477](./IROS2019速读(四).assets/image-20191222123748477.png)

1. 利用VAE对state图像进行表征

2. 创建一个state-transition model，以$s_t, a_t$作为输入，以$s_{t+1}$作为输出，使其成为action-conditioned  $s_{t+1}=f\left(s_{t}, a_{t}\right)=F\left(\left[s_{t}, G\left(a_{t}\right)\right]\right)$

3. 损失函数如下, 其中预测的state representation为 $I_{t+1}'=Dec(f(Enc(I_t),a_t))$
   $$
   \begin{aligned} \mathcal{L}_{f} &=\left\|f\left(\operatorname{Enc}\left(I_{t}\right), a_{t}\right)-\operatorname{Enc}\left(I_{t+1}\right)\right\|_{2} \\ &+\left\|\operatorname{Dec}\left(f\left(\operatorname{Enc}\left(I_{t}\right), a_{t}\right)\right)-I_{t+1}\right\|_{2} \end{aligned}
   $$

4. 总损失为
   $$
   \mathcal{L}=\mathcal{L}_{VAE}+\lambda \mathcal{L}_f
   $$

![image-20191222144804275](.\IROS2019速读(四).assets\image-20191222144804275.png)

### Opinion

其实我一直认为像VAE，GAN这种生成网络是可以用于RL提升data efficiency的，这篇文章确实在像这个方向做，但是GAN本身在实际使用时训练时间过长，消耗大量资源，所以对RL来说是利是弊还得看具体使用。



今天就先两更啦，明儿见

*Originally published at* [*https://github.com/Skylark0924/Reinforcement-Learning-in-Robotics/*](https://github.com/Skylark0924/Reinforcement-Learning-in-Robotics/blob/master/Related Works/Overcoming Exploration in Reinforcement Learning with Demonstrations.md)*.*

