# IROS2019 Paper速读（四）

## Cascaded Gaussian Processes for Data-efficient Robot Dynamics Learning

Paper: http://arxiv.org/abs/1910.02291

### Keywords

cascaded Gaussian processes, inverse dynamics learning

### Main Idea

提出了一种级联高斯过程用于机器人逆运动学的学习，通过使用在一个关节上训练的GP的输出来替代所有先前的关节状态，来递归地减小回归问题的尺寸，从而减小特征空间的尺寸。级联GP由递归NE(Newton-Euler)方程启发.

![image-20191222112555208](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(四).assets\image-20191222112555208.png)

Standard GP: 使用所有关节的states进行训练

Cascaded GP: 用关节转矩表征关节states

由于使用欧拉运动公式需要每个关节都有力/力矩传感器，所以本文假设使用关节扭矩就可以在某种程度上总结外部关节的影响。

Cascaded GP的inward recursion，相应GP预测出的关节转矩 $\tau_{i+1}$ 会被用于代替外部关节( $i+1\sim N$) 的states

outward recursion则从机器人基座开始，以同样的道理代替内部关节($1\sim i+1$)的states，从而降低维度。

除此之外，还是用 Derivative-Free Features 作为解决数值微分嘈杂本质的一种手段，这是获得联合速度和加速度的必然步骤。


### Opinion

文中建议考虑使用semi-parametric, derivative-based feature结合cascaded approach，这样可以提高模型的generalization。由于我对这个方向不太熟，就不做过多评论了。



## Learning Real-World Robot Policies by Dreaming

Paper: http://arxiv.org/abs/1805.07813

Website: https://piergiaj.github.io/robot-dreaming-policy/

### Keywords

data efficiency, real-world, dreaming model(world model)

### Main Idea

设计了一个dreaming model，使机器人在其中进行interaction，而不是直接和real-world。这个思想和上上期的Learning Physics-Based Manipulation in Clutter: Combining Image-Based Generalization and Look-Ahead Planning有些相似。

Dreaming Model 由 FCNN, Variational Autoencoder, action-conditioned future regressor(ACFR)构成。

ACFR: 可以模拟机器人执行指令action之后的state变化。这就意味着，Dreaming Model相较于之前的Model-based方法，引入了 imagined trajectories 来代替之前的 real trajectories，这也是作者 use the word 'dreaming' rather than 'model-based' 的用意。详见reddit上的[debate](https://www.reddit.com/r/reinforcementlearning/comments/8rmcxn/learning_realworld_robot_policies_by_dreaming/).

以下是dreaming生成的imagine trajectories的可视化：

![](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(四).assets\dream_2.gif)

![](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(四).assets\dream_6.gif)

It is really awesome, isn't it?

那我们接下来看一下如此marvelous的dreaming是如何实现的吧！
$$
\begin{array}{c}{\text { State Encoder } E n c_{\theta}: I \mapsto s} \\ {\text { State Decoder } D e c_{\theta}: s \mapsto I} \\ {\text { State-transition } f_{\theta}: s_{t}, a_{t} \mapsto s_{t+1}} \\ {\text { Reward/End Detector } R_{\theta}: s \mapsto r}\end{array}
$$
![image-20191222123748477](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(四).assets\image-20191222123748477.png)



### Opinion