# IROS2019速读(二)

## Learning Physics-Based Manipulation in Clutter: Combining Image-Based Generalization and Look-Ahead Planning

Paper: http://arxiv.org/abs/1904.02223

Video: https://youtu.be/EmkUQfyvwkY

### Keywords

end-to-end learning, planning-based look-ahead, sequential decision, generalization, motion planning

### Main Idea

Base Model: Domain Randomization + State Representation + (CNN+DNN) + $\epsilon$-RHP(Receding Horizon Planning) + Mask RCNN

![image-20191219185107956](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读(二).assets\image-20191219185107956.png)

简述一下本文的流程：

1. 将图像数据使用Mask RCNN进行实例分割（instance segmentation)
2. 使用域随机化(domain randomization)用以提升Value Function的泛化能力，使其不受到以下因素的影响：the shape and scale of the objects, the clutter density, the target location, and the initial pose of the objects and end-effector.
3. 在机器人的大脑中创建一个想象空间（simulator by Box2D)
4. 由CNN+DNN构造的Value Function网络，采用double Q-learning学习
5. 利用RHP探索策略在simulator中进行rollout
6. 将概率最大的动作输出并在real-world robot上执行
7. 返回1

### Opinion

Awesome work! 通过实时的simulator避免了以往先sim后real的问题，giving robotics the ability of thinking and planning in their mind! 通过语义分割以及状态表征，使机器人能够利用物体的形状来降低抓取难度。



## Motion Planning for a Continuum Robotic Mobile Lamp: Defining and Navigating the Configuration Space

### Keywords

continuum robots, [RRT](https://www.cnblogs.com/21207-iHome/p/7210543.html)(rapidly exploring random tree)

### Main Idea

Main Contribution:

1. 连续体机器人的位形空间(configuration space)分析（single section) :milky_way:
2. 连续体与刚体机器人的位形空间比较
3. 连续体机器人的几何约束

### Opinion

扩展了连续体机器人的运动规划理论



## Soft Action Particle Deep Reinforcement Learning for a Continuous Action Space

Code: https://github.com/rllab-snu/soft_action_particle_method

Video: https://www.youtube.com/watch?v=4PdSHtH_cB4

### Keywords

on-policy, particle

### Main Idea

现今的AC算法需要大量的参数，导致过拟合且不易于调参。本文提出了一种新的AC架构，将传统AC中Actor网络用一组action particles代替，策略概率由state action value network表示。

1. 将动作空间分段成 $N_A$ 个高斯分布： $M=\left\{\mu_{i}\right\}_{i=1}^{N_{A}}, S=\left\{\sigma_{i}\right\}_{i=1}^{N_{A}}$

2. 在训练过程中，agent总是sample新的action particle set $A$:

$a_{i}=\mu_{i}+\epsilon_{i} \sigma_{i}, \quad \epsilon_{i} \sim \mathcal{N}(0,1)$

3. Policy distribution 定义如下，

$$
\pi_{\theta}\left(a_{i} | s\right) \triangleq \frac{\mathbb{E}_{\epsilon_{i}}\left[\exp \left(Q_{\theta}\left(s, a_{i}\right) / \alpha\right)\right]}{\sum_{m_{j} \in M, \sigma_{j} \in S} \mathbb{E}_{\epsilon_{j}}\left[\exp \left(Q_{\theta}\left(s, a_{j}\right) / \alpha\right)\right]}
$$

4. 使用 Soft Q-learning 更新 Q function
5. 为了防止最优动作不在 $M$ 中，action particle也应该朝着最大动作值的方向移动
6. 为了防止action particle 在更新后聚集在一起，通过计算两两之间的欧氏距离，将距离小于阈值的其中一个删除，然后进行重新采样（Resampling)

### Opinion

这算是进化算法和RL的结合，粒子群算法和SAC的结合，大大减少了参数量，在保证了精度的情况下，不失为一种好办法。