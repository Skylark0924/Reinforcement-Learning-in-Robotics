# IROS2019 Paper速读（一）

## Robot Learning of Shifting Objects for Grasping in Cluttered Environments

Lars Berscheid, Pascal Meißner, and Torsten Kröger

Video and Code: https://github.com/pantor/learning-shifting-for-grasping.

### Keywords

Shifting, real-world, sparse rewards

### Main Idea

state是深度图像信息，经过**卷积层**，得到action组 (x, y, a, d) 的Q值。x, y为距离objects的坐标，a为工具坐标系关于global z轴的倾角，d为离散运动基元的指针。由于是卷积计算，最后得到的输出是对于每一个运动基元都有32000个（40x40x20）action组的Q值。最后得到以下四点信息：

1. 对给定的(x,y,a)估计抓取的可能性 $\psi$
2. 计算最佳抓取状态(x,y,a,d)
3. 计算在箱子中最大化全局可能性 $\hat{\psi}$
4. 对给定的(x,y,a)以及给定的周边范围，计算这个区域的最大化抓取可能性 $\hat{\psi}_w(s,x,y,a)$

此外，设计第二个RL，用以学习shifting并克服sparse rewards问题：
$$
r_s(s)=\dfrac{1}{2}(\hat{\psi}_w(s',x,y,a)-\hat{\psi}_w(s,x,y,a)+1)
$$
Obviously, 可以学到某时刻的shifting是否对抓取概率起到正面作用。最后用阈值 $\rho$ 判断箱子是否为空。

![image-20191216203301798](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191216203301798.png)

### Opinion

这种手调重要超参（$\psi$, $\rho $）的方式真的不适合叫做Adaptive Control，仿佛披着RL皮的分段PID. 不过克服sparse rewards的方式还是有可取之处的。



## Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation

Paper: http://arxiv.org/abs/1904.06703

### Keywords

Hierarchical RL, sparse rewards

### Main Idea

Base model: DDPG + HER + HRL

- high level agent: 将目标细分成阶段目标 $sg\leftarrow \pi_1(s,g)$

- low level agent: DDPG + HER reach 阶段目标 $a_t\leftarrow \pi_0(s||sg)$

observations ($o_i$) represented in blue, sub-goal($sg_i$) in green and the final goal($g$) in red; $ag_i$ refers to achieved goals

At first episode, $sg_i=ag_i+\mathcal{N}(0,\sigma)$

![image-20191216212441606](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191216212441606.png)



![image-20191216211111079](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191216211111079.png)

文中提到，使用DtD方法，使agent理解了环境的距离表征。

### Opinion

思路其实和HER差不多，都是给出阶段目标，只不过这篇文章使用HRL将阶段目标体现得更明显。



## Sample-efficient Deep Reinforcement Learning with Imaginary Rollouts for Human-Robot Interaction

Paper: http://arxiv.org/abs/1908.05546

### Keywords

model-based, Human-Robot interaction, generate synthetic transitions, accelerate learning

### Main Idea

Base model: DQN + VAE + MDN(Mixture Density Networks)

# IROS2019 Paper速读

## Robot Learning of Shifting Objects for Grasping in Cluttered Environments

Lars Berscheid, Pascal Meißner, and Torsten Kröger

Video and Code: https://github.com/pantor/learning-shifting-for-grasping.

### Keywords

Shifting, real-world, sparse rewards

### Main Idea

state是深度图像信息，经过**卷积层**，得到action组 (x, y, a, d) 的Q值。x, y为距离objects的坐标，a为工具坐标系关于global z轴的倾角，d为离散运动基元的指针。由于是卷积计算，最后得到的输出是对于每一个运动基元都有32000个（40x40x20）action组的Q值。最后得到以下四点信息：

1. 对给定的(x,y,a)估计抓取的可能性 $\psi$
2. 计算最佳抓取状态(x,y,a,d)
3. 计算在箱子中最大化全局可能性 $\hat{\psi}$
4. 对给定的(x,y,a)以及给定的周边范围，计算这个区域的最大化抓取可能性 $\hat{\psi}_w(s,x,y,a)$

此外，设计第二个RL，用以学习shifting并克服sparse rewards问题：
$$
r_s(s)=\dfrac{1}{2}(\hat{\psi}_w(s',x,y,a)-\hat{\psi}_w(s,x,y,a)+1)
$$
Obviously, 可以学到某时刻的shifting是否对抓取概率起到正面作用。最后用阈值 $\rho$ 判断箱子是否为空。

![image-20191216203301798](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191216203301798.png)

### Opinion

这种手调重要超参（$\psi$, $\rho $）的方式真的不适合叫做Adaptive Control，仿佛披着RL皮的分段PID. 不过克服sparse rewards的方式还是有可取之处的。



## Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation

Paper: http://arxiv.org/abs/1904.06703

### Keywords

Hierarchical RL, sparse rewards

### Main Idea

Base model: DDPG + HER + HRL

- high level agent: 将目标细分成阶段目标 $sg\leftarrow \pi_1(s,g)$

- low level agent: DDPG + HER reach 阶段目标 $a_t\leftarrow \pi_0(s||sg)$

observations ($o_i$) represented in blue, sub-goal($sg_i$) in green and the final goal($g$) in red; $ag_i$ refers to achieved goals

At first episode, $sg_i=ag_i+\mathcal{N}(0,\sigma)$

![image-20191216212441606](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191216212441606.png)



![image-20191216211111079](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191216211111079.png)

文中提到，使用DtD方法，使agent理解了环境的距离表征。

### Opinion

思路其实和HER差不多，都是给出阶段目标，只不过这篇文章使用HRL将阶段目标体现得更明显。



## Sample-efficient Deep Reinforcement Learning with Imaginary Rollouts for Human-Robot Interaction

Paper: http://arxiv.org/abs/1908.05546

### Keywords

environment model, Human-Robot interaction, generate synthetic transitions, accelerate learning

### Main Idea

Base model: DQN + VAE + MDN(Mixture Density Networks)

To be honest, this is the first time I heard MDN:

MDN: MDN的输出是混合高斯分布，用以解决多值函数拟合问题，例如一些反函数和随机过程。混合概率分布可以得到每个x对应的多个y的值和对应的概率，借此即可拟合多值函数。其网络输出包含三部分：$\mu, \sigma$，对应高斯分布在混合时的权重 $\alpha$。

The proposed architecture consists of three components:

1. the **vision module** (V) that produces abstract representations of input images -> **VAE**
2. the **environment model** (M) which generates imaginary rollouts and predicts future states $z_{t+1}$ and the reward rt based on current states $z_t$ and input actions $a_t$. 
   - MDN: learns the conditional probability distribution of the next state $p(z_{t+1}|z_t, a_t)$. 好处在于可以学习随机过程，毕竟人机协作时人类的反应是不可预测的。
   - r-network: learns to predict the reward for each state
   - d-network: learns to predict whether a state is terminal or not
3. the **controller** (C) that learns to map states into actions. -> **DQN**，注意：输出的action只有六种，三个卡牌顺时针或逆时针旋转90度

![image-20191217203423494](D:\Github\Reinforcement-Learning-in-Robotics\Related Works\IROS2019速读.assets\image-20191217203423494.png)

该框架尝试学习环境模型并产生模拟数据供controller学习

### Opinion

实际上，这个工作好像和机器人没啥关系，还是属于RL传统的游戏领域，毕竟也没有输出电机的转矩，完全model-based的框架。对于机器人本身并没有过多的关注，研究的是high-level决策，而将机器人作为一个整体的执行器。