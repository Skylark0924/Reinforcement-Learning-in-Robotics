## Probabilistic in Robotics Ⅱ: Bayesian Estimation/Inference

**统计推断**旨在根据可观察到的事物来了解不可观察到的事物。即，统计推断是基于一个总体或一些样本中的某些观察变量（通常是影响）得出结论的过程，例如关于总体或样本中某些潜在变量（通常是原因）的准时估计、置信区间或区间估计等。

### 贝叶斯估计/推断

我们在上一章强调贝叶斯派与频率派世界观差异的时候，着重描述了贝叶斯派对于**参数是随机变量**的看法，然而最大后验概率估计MAP得到的值却是个定值 $\theta^*$，是参数这个变量概率分布中的一个特定点。这听起来没有很好的贯彻贝叶斯派的精神。

**贝叶斯估计**是个更彻底的贝叶斯派，是MAP的进一步扩展。不再估计一个特定的参数 $\theta$，而是要估计它的**分布**。

在贝叶斯估计中，样本经验分布 $P(X)$ 不再被省略，因为这不再是专门对参数 $\theta$ 的估计。

离散型贝叶斯公式：
$$
P(\theta | X)=\frac{P(X | \theta) P(\theta)}{P(X)}
$$
连续型贝叶斯公式：
$$
P(\theta | X)=\frac{P(X | \theta) P(\theta)}{\int_{\Theta} P(X | \theta) P(\theta) d \theta}
$$
很明显，这并不是一个好处理的公式，尤其是连续型的分母（**归一化因子**） $P(X)=\int_{\Theta} P(X | \theta) P(\theta) d \theta$ (全概率公式展开，还记得吗🧐)

### 手算贝叶斯估计

我们先试着做做，还是之前的扔硬币，不过样本变了点，正六反四：

以下内容来自[贝叶斯估计、最大似然估计、最大后验概率估计](https://link.zhihu.com/?target=http%3A//noahsnail.com/2018/05/17/2018-05-17-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/)

使用共轭先验分布，就可以更好的解决这个问题。二项分布参数的共轭先验是Beta分布，由于 θ 的似然函数服从二项分布，因此在贝叶斯估计中，假设 θ 的先验分布服从 $P(\theta) \sim \operatorname{Beta}(\alpha, \beta)$，Beta分布的概率密度公式为：
$$
f(x ; \alpha, \beta)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}
$$
因此，贝叶斯公式可写作：
$$
\begin{aligned}
P(\theta | X) &=\frac{P(X | \theta) P(\theta)}{\int_{\Theta} P(X | \theta) P(\theta) d \theta} \\
&=\frac{\theta^{6}(1-\theta)^{4} \frac{\theta^{2-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)}}{\int_{\Theta} \theta^{6}(1-\theta)^{4} \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)} d \theta} \\
&=\frac{\theta^{\alpha+6-1}(1-\theta)^{\beta+4-1}}{\int_{\Theta} \theta^{\alpha+6-1}(1-\theta)^{\beta+4-1} d \theta} \\
&=\frac{\theta^{\alpha+6-1}(1-\theta)^{\beta+4-1}}{B(\alpha+6-1, \beta+4-1)} \\
&=\operatorname{Beta}(\theta | \alpha+6-1, \beta+4-1) \\
&=\operatorname{Beta}(\theta | \alpha+6, \beta+4)
\end{aligned}
$$
从上面的公式可以看出，$P(\theta | X) \sim \operatorname{Beta}(\theta | \alpha+6, \beta+4)$。其中 B 函数，也称Beta函数，是一个标准化常量，用来使整个概率的积分为1。$Beta(θ|α+6,β+4)$就是贝叶斯估计的结果。

如果使用贝叶斯估计得到的 θ 分布存在一个有限均值，则可以用后验分布的期望作为 θ 的估计值。假设$α=3,β=3$，在这种情况下，先验分布会在0.5处取得最大值，则$P(θ|X)∼Beta(θ|9,7)$，Beta(θ|9,7)的曲线如下图：

![Beta(9,7)](http://noahsnail.com/images/blog/Beta_9_7.png)

从上图可以看出，在$α=3,β=3$的情况下，θ的估计值 $\hat{\theta}$ 应该在0.6附近。根据Beta分布的数学期望公式 $E(θ)=\dfrac{α}{α+β}$ ，我们可以和MAP一样求出一个参数的特值：
$$
\hat{\theta}=\int_{\Theta} \theta P(\theta | X) d \theta=E(\theta)=\frac{\alpha}{\alpha+\beta}=\frac{9}{9+7}=0.5625
$$
**求解步骤：**

- 确定参数的似然函数
- 确定参数的先验分布，应是后验分布的共轭先验
- 确定参数的后验分布函数
- 根据贝叶斯公式求解参数的后验分布



### 对测试集的估计

这个功能是贝叶斯估计独有的。贝叶斯估计要解决的不是如何估计参数，而是用来估计新测量数据出现的概率，对于新出现的数据 $\tilde x$ ：
$$
P(\tilde{x} | X)=\int_{\Theta} P(\tilde{x} | \theta) P(\theta | X) d \theta=\int_{\Theta} P(\tilde{x} | \theta) \frac{P(X | \theta) P(\theta)}{P(X)} d \theta
$$


**那么实际项目中，我们如何计算贝叶斯估计呢？**

- 基于**采样**的**马尔可夫链蒙特卡罗**(Markov Chain Monte Carlo，简称MCMC)方法
- 基于**近似**的**变分推断**(Variational Inference，简称VI)方法

本文主要介绍MCMC方法，其余的会在后续文章出现。

### 采样 —— 一种近似技术

当我们需要以较小的代价近似许多项的和或某个积分时，**采样**是一种很灵活且明智的选择。**所以这不只是贝叶斯推断特有的方法。**

#### 马尔可夫链蒙特卡洛 MCMC

先来看看这个名字：

- 蒙特卡洛：指采样的目的
- 马尔可夫链：指如何获得这些样本

![img](./PR Ⅲ Bayesian_MCMC.assets/1_3nBb4AqcriLcENdpBp4fpQ@2x.png)

为了避免处理棘手的后验分布计算，我们从分布中采样，并用这些样本来计算分布的统计参数。

**那么MCMC到底是什么思路呢？**

1. 建立一个马尔科夫链，从其平稳分布中获得样本；
2. 从马尔可夫链中模拟随机的状态序列，该序列足够长，能够（几乎）达到稳态；
3. 保留生成的一些状态作为样本

![img](./PR Ⅲ Bayesian_MCMC.assets/1_AZBh2kDanLoTFmb3yzErGQ@2x.png)

> 是不是还是一脸懵，别急，慢慢来，我们一步步分析。
>
> 点个赞啊亲，写的很累的。

**1. 马尔科夫链怎么建？**

马尔可夫链我们做强化学习的都熟悉，最常用的性质就是**马尔可夫性（无后效性）**。但是跟我们的线性链不同，这里是完整的有向图。

MCMC的马尔科夫链指的是**建立下一时刻的样本的采样与当前时刻的样本之间的关系 ———— 转移概率矩阵** $P_{ij}(x_{t+1}|x_t)$，$P_{ij}$ 中的每个元素代表了变量X从状态i转移到另一个状态j的概率。

马尔科夫链另一个神奇的性质 ———— **收敛性**：对于非周期的马尔可夫链来说（非周期的意思就是状态转移不会存在闭环，即永远只在几个有限的状态之间绕圈子），如果任意两个状态都是连通的，（即不会存在某个状态无法转移到另一个状态），那么给定一个初始的概率分布样本，通过我状态转移矩阵的一系列转换，都会收敛到某个稳定的概率分布，这个稳定的概率分布与初始概率无关。

![img](https://pic2.zhimg.com/80/v2-8b78f1035471eab24c809d6fb7dc1b01_1440w.png)

**从数学角度看**：以任意的初始样本 $x_0$ 开始，根据马尔可夫链的收敛性质，经过一段时间的转移概率的连乘后，假设为n次，x的采样概率都会收敛到一个**平稳分布**，设为 $\pi(x)$，即后续的所有样本均服从这个概率分布 $\pi(x)$。
$$
\begin{aligned}
\pi_{t+1}\left(x^{*}\right)=& \int_{x} \pi_{t}(x) P\left(x^{*} | x\right) d x \\
& t>n
\end{aligned}
$$


## Rreference

1. [Bayesian inference problem, MCMC and variational inference](https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29)